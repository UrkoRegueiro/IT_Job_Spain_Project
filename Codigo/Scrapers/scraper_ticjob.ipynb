{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114ec7bd-ccee-4d93-849b-698a9667862f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from librerias_funciones import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1384de7b-2469-4e83-8ced-e962cf0fcc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para manejar la selección\n",
    "def handle_variable_selection(change):\n",
    "    global intervalo_temporal_busqueda\n",
    "    intervalo_temporal_busqueda = change['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5589fed6-1360-435b-9b64-bfe871a706f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://ticjob.es/esp/busqueda'\n",
    "portal_busqueda = 'ticjob'\n",
    "\n",
    "usuario = 'bruno.octavio.tomas@gmail.com'\n",
    "password = 'password123'\n",
    "trabajor_a_buscar = 'Data Scientist'\n",
    "ubicacion_a_buscar = 'España'\n",
    "nombres_columnas = [\"titulo\", \"empresa\", \"fecha\", \"herramientas\", \"descripcion\", \"ubicacion\", \n",
    "                    \"experiencia\", \"tipo_contrato\", \"salario\", \"fecha_scrapeo\",\"url\", \"portal\"]\n",
    "\n",
    "intervalo_temporal_busqueda = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83498bc1-c6ca-44e8-85e1-f5078bab4133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intervalo_temporal_busqueda = timedelta(days = intervalo_temporal_busqueda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee45f70-285f-4ec2-8c8f-7f9512a428a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "sleep(1)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb238898-044f-4e46-8205-4365761152e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seleccionamos el puesto a buscar\n",
    "#driver.find_element(By.ID, 'keywords-input').send_keys(trabajor_a_buscar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e15f09-a71c-4fe7-a4fa-7ab7eb00f423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#seleccionamos la ubicacion a buscar\n",
    "\n",
    "#driver.find_element(By.ID, 'location-map-dd').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf32c36-4a11-49f2-b537-12104384fec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ordenamos los resultados por fecha\n",
    "\n",
    "driver.find_element(By.CLASS_NAME, 'sort-by-date-container').click()\n",
    "\n",
    "sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de5262-35a0-404e-aaa6-1d1bce122eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fecha_scrapeo = datetime.now().date()\n",
    "\n",
    "while True:\n",
    "    data = []\n",
    "    ofertas = driver.find_elements(By.CLASS_NAME, 'job-card')\n",
    "\n",
    "    for oferta in ofertas:\n",
    "        #Extraemos los datos con selenium, ya que al obtener la url de la pagina nos da los de una busqueda generica sin nuetros criterios\n",
    "        #Extraemos solo los datos del intervalo temporal seleccionado\n",
    "        fecha = oferta.find_element(By.CSS_SELECTOR, 'div[class = \"job-card-label date-field\"]').text\n",
    "        fecha = datetime.strptime(fecha, '%d/%m/%Y').date()\n",
    "\n",
    "        if fecha < fecha_scrapeo - intervalo_temporal_busqueda:\n",
    "            break\n",
    "\n",
    "        url = oferta.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        #Extraemos los datos con soup\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        titulo = extraccion_datos_try_except(soup, 'h1', {'id' : 'job-title'})\n",
    "        \n",
    "        empresa = soup.find('a', class_ = 'company-image')['title']\n",
    "        \n",
    "        descripcion = extraccion_datos_try_except(soup, 'div', {'class' : 'job-offer job-offer-content'})\n",
    "        \n",
    "        ubicacion = extraccion_datos_try_except(soup, 'li', {'class' : 'multi-job-location-apply'})\n",
    "        if (ubicacion != np.nan) and (';' in ubicacion):\n",
    "            ubicacion = ubicacion.split(';')\n",
    "        \n",
    "        experiencia = extraccion_datos_try_except(soup, 'li', {'id' : 'summaryExp'})\n",
    "\n",
    "        localizacion = soup.find('li', class_ = 'multi-job-location-apply')\n",
    "        tipo_contrato = localizacion.find_next('li').get_text(strip = True)\n",
    "\n",
    "        salario = extraccion_datos_try_except(soup, 'li', {'id' : 'summarySalary'})\n",
    "        # Eliminamos los salarios == '0' ya que no son datos reales que se muestren en la pagina, ya que corresponden con campos vacios\n",
    "        salario = salario if salario != '0' else np.nan\n",
    "        \n",
    "        herramientas = soup.find('div', class_ = 'search-criteria-tags').find_all('a')\n",
    "        herramientas = [herramienta.text for herramienta in herramientas]\n",
    "        \n",
    "        data.append([titulo, empresa, fecha, herramientas, descripcion, ubicacion, experiencia, tipo_contrato, salario, fecha_scrapeo, url, portal_busqueda])\n",
    "        sleep(1)\n",
    "        \n",
    "    \n",
    "    #Guardamos cada vez que termina de sacrapear una pagina\n",
    "    guardar_datos(portal_busqueda = portal_busqueda, nombres_columnas = nombres_columnas, data = data)\n",
    "    \n",
    "    paginas_totales = driver.find_element(By.CLASS_NAME, 'page-list').text.split('\\n')[-1]\n",
    "    paginas_totales = int(paginas_totales)\n",
    "    pagina_actual = int(driver.find_element(By.CLASS_NAME, 'current').text)\n",
    "\n",
    "    # Pasamos a la siguiente pagina hasta llegar a la ultima\n",
    "    if paginas_totales != pagina_actual:\n",
    "        # Pasamos a la siguiente pagina\n",
    "        siguiente_pagina = driver.find_element(By.CLASS_NAME, 'next')\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", siguiente_pagina)\n",
    "        siguiente_pagina.click()\n",
    "        sleep(2)\n",
    "    else:\n",
    "        driver.quit()\n",
    "        print('Scrapeo completado')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d0a220-c99d-4576-b5ea-66685d94f277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Actualizamos el fichero de control de extraccion de datos\n",
    "actualizar_fichero_control_extracciones_datos(portal_busqueda, fecha_scrapeo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
