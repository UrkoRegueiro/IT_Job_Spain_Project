{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05cda5f0-8145-4470-b5db-39bd4f8482f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Resolución captcha:\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Probar get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6977b4c-28e6-497f-a8f2-0b7a003888e2",
   "metadata": {},
   "source": [
    "# 0. Funciones Captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281283fe-aaf7-4512-8163-b18b47ada00f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def record_audio(duration=5, sample_rate=44100):\n",
    "    print(\"Recording...\")\n",
    "\n",
    "    # Record audio\n",
    "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=2, dtype=np.int16)\n",
    "    sd.wait()\n",
    "\n",
    "    print(\"Recording finished.\")\n",
    "\n",
    "    return audio_data\n",
    "\n",
    "def save_audio(audio_data, file_path=\"recorded_audio.mp3\", sample_rate=44100):\n",
    "    print(\"Saving audio...\")\n",
    "\n",
    "    # Save audio to file using scipy.io.wavfile.write\n",
    "    write(file_path, sample_rate, audio_data)\n",
    "\n",
    "    print(f\"Audio saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e894b7-8bb0-4ba1-8837-db91ceb4a6ac",
   "metadata": {},
   "source": [
    "# 1. Infoempleo Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d962e-4e70-4405-a57a-184f56accde6",
   "metadata": {},
   "source": [
    "## 1.1. URL ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ce8962-ca71-4f70-8484-998c588e5277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def url_scraper_infoempleo(url, opts, nuevas= True):\n",
    "    \n",
    "    browser = webdriver.Chrome(options=opts)\n",
    "    browser.maximize_window()\n",
    "    \n",
    "    browser.get(url)\n",
    "    sleep(2)\n",
    "    \n",
    "    # Acepto cookies\n",
    "    browser.find_element(By.ID, \"onetrust-accept-btn-handler\").click()\n",
    "    sleep(1)\n",
    "    \n",
    "    url_empleos = []\n",
    "    while True:\n",
    "\n",
    "        try:\n",
    "            browser.find_element(By.ID, \"lightbox\").find_element(By.CLASS_NAME, \"close\").click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if nuevas:\n",
    "            # Boton ofertas en las últimas 24h:\n",
    "            browser.find_element(By.ID, \"fechapublicacion1\").click()\n",
    "            sleep(1)\n",
    "\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        empleos = soup.find(\"div\", class_= \"main-content\").find_all(\"li\", class_= \"offerblock\")\n",
    "\n",
    "        for url in empleos:\n",
    "            try:\n",
    "                url_oferta = \"https://www.infoempleo.com\" + url.find(\"a\")[\"href\"]\n",
    "                url_empleos.append(url_oferta)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            # Hago scroll hasta el final:\n",
    "            elemento_objetivo = browser.find_elements(By.CLASS_NAME, \"related-offer-item\")[-1]\n",
    "            browser.execute_script(\"arguments[0].scrollIntoView(true);\", elemento_objetivo)\n",
    "            sleep(1)\n",
    "            # Siguiente página:\n",
    "            browser.find_element(By.CLASS_NAME, \"pagination\").find_element(By.CLASS_NAME, \"next\").click()\n",
    "            sleep(2)\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    return url_empleos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e101433-d023-406a-9cc7-2d9375743ac3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2. Información ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44bf77e1-aad8-44dd-a615-bbd96af2d5d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraper_infoempleo(url_empleos, opts):\n",
    "    # Inicio browser:\n",
    "    browser = webdriver.Chrome(options=opts)\n",
    "    browser.maximize_window()\n",
    "    \n",
    "    # Valores fijos:\n",
    "    portal = \"infoempleo\"\n",
    "    fecha_scrapeo = datetime.datetime.now().date()\n",
    "    \n",
    "    \n",
    "    datos_ofertas_empleo = []\n",
    "    contador = 0\n",
    "    for empleo in url_empleos:\n",
    "    \n",
    "        browser.get(empleo)\n",
    "        sleep(2)\n",
    "\n",
    "        try:\n",
    "            # Acepto cookies\n",
    "            browser.find_element(By.ID, \"onetrust-accept-btn-handler\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Cierro popup si aparece:\n",
    "        try:\n",
    "            browser.find_element(By.ID, \"lightbox\").find_element(By.CLASS_NAME, \"close\").click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        soup_oferta = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            titulo = soup_oferta.find(\"div\", class_= \"title-wrapper\").find(\"h1\").text\n",
    "\n",
    "        except:\n",
    "            titulo = np.nan\n",
    "\n",
    "        try:\n",
    "            empresa = soup_oferta.find(\"div\", class_= \"title-wrapper\").find(\"li\", class_= \"companyname\").text\n",
    "\n",
    "        except:\n",
    "            empresa = np.nan\n",
    "\n",
    "        try:\n",
    "            presencialidad = soup_oferta.find(\"div\", class_= \"title-wrapper\").find(\"li\", class_= \"badge\").text\n",
    "\n",
    "        except:\n",
    "            presencialidad = np.nan    \n",
    "\n",
    "        try:\n",
    "            fecha = soup_oferta.find(\"div\", class_= \"title-wrapper\").find(\"li\", class_= \"mt10\").text.strip()\n",
    "\n",
    "        except:\n",
    "            fecha = np.nan\n",
    "\n",
    "        try:\n",
    "            ubicacion = soup_oferta.find(\"div\", class_= \"title-wrapper\").find(\"li\", class_= \"block\").text.strip()\n",
    "\n",
    "        except:\n",
    "            ubicacion = np.nan\n",
    "\n",
    "        # Bloque de características:\n",
    "\n",
    "        try:\n",
    "            browser.find_element(By.CLASS_NAME, \"areapos-vmore\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        bullet_points = soup_oferta.find(\"div\", class_= \"offer-excerpt\").find_all(\"ul\", class_= \"inline\")\n",
    "\n",
    "        try:\n",
    "            experiencia = bullet_points[0].find_all(\"p\")[0].text\n",
    "        except:\n",
    "            experiencia = np.nan\n",
    "\n",
    "        try:\n",
    "            salario = bullet_points[0].find_all(\"p\")[1].text\n",
    "        except:\n",
    "            salario = np.nan\n",
    "\n",
    "        try:\n",
    "            funciones = [funcion.text for funcion in bullet_points[1].find_all(\"li\")[1:]]\n",
    "        except:\n",
    "            funciones = np.nan\n",
    "\n",
    "        try:\n",
    "            solicitudes = bullet_points[2].find_all(\"p\")[1].text\n",
    "        except:\n",
    "            solicitudes = np.nan        \n",
    "\n",
    "        try:\n",
    "            tipo_contrato = bullet_points[3].find_all(\"p\")[0].text\n",
    "        except:\n",
    "            tipo_contrato = np.nan\n",
    "\n",
    "        try:\n",
    "            jornada = bullet_points[3].find_all(\"p\")[1].text\n",
    "        except:\n",
    "            jornada = np.nan  \n",
    "\n",
    "        cuerpo_oferta = soup_oferta.find(\"div\", class_= \"offer\").find_all(\"pre\")\n",
    "\n",
    "        try:\n",
    "            descripcion = cuerpo_oferta[0].text.replace(\"\\n\", \"\").replace(\"*\", \"\").strip()\n",
    "        except:\n",
    "            descripcion = np.nan    \n",
    "\n",
    "        try:\n",
    "            herramientas = cuerpo_oferta[1].text.replace(\"\\n\", \"\").replace(\"*\", \"\").strip()\n",
    "        except:\n",
    "            herramientas = np.nan\n",
    "\n",
    "        datos_ofertas_empleo.append([titulo, empresa, fecha, herramientas, descripcion, ubicacion, presencialidad, funciones, jornada, experiencia, tipo_contrato, salario, solicitudes, fecha_scrapeo, empleo, portal])\n",
    "        contador += 1\n",
    "\n",
    "        if contador % 100 == 0:\n",
    "            sleep(20)\n",
    "            \n",
    "            \n",
    "    df = pd.DataFrame(datos_ofertas_empleo, columns= [\"titulo\", \"empresa\", \"fecha\", \"herramientas\", \"descripcion\", \"ubicacion\", \"presencialidad\", \"funciones\", \"jornada\", \"experiencia\", \"tipo_contrato\", \"salario\", \"solicitudes\", \"fecha_scrapeo\", \"url\", \"portal\"])\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e0709-5566-4b53-94b1-16fb7453e581",
   "metadata": {},
   "source": [
    "# 2. Talenthacker Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bca9fe-24c6-4585-8652-6d519153ae78",
   "metadata": {},
   "source": [
    "## 2.1. URL ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d21b97-0016-4a77-b652-ce0ec25c0f73",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def url_scraper_talenthacker(url, opts):\n",
    "    \n",
    "    # Inicio navegador\n",
    "    browser = webdriver.Chrome(options=opts)\n",
    "    browser.maximize_window()    \n",
    "    browser.get(url)\n",
    "    sleep(2)\n",
    "    \n",
    "    # Acepto cookies:\n",
    "    browser.find_element(By.CLASS_NAME, \"q-banner__actions\").find_element(By.CLASS_NAME, \"buttons-wrapper\").find_element(By.CLASS_NAME, \"q-btn--standard\").click()\n",
    "\n",
    "    # Hago scroll hasta poder ver todas las ofertas disponibles:\n",
    "    last_height = browser.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    while True:\n",
    "        # Scroll:\n",
    "        browser.execute_script(\"window.scrollTo(0,document.documentElement.scrollHeight);\")\n",
    "        sleep(3)\n",
    "        # Calcula la altura del nuevo scroll y la compara con la del último:\n",
    "        new_height = browser.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            print(\"Ya no hay más página.\")\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "    talent_hacker = \"https://talenthackers.net\"\n",
    "    ofertas = soup.find(\"div\", class_= \"jobs-list-wrapper\").find_all(\"div\", class_= \"col-12\")\n",
    "\n",
    "    url_ofertas = []\n",
    "    for oferta in ofertas:\n",
    "        url_oferta = talent_hacker + oferta.find(\"a\")[\"href\"]\n",
    "        url_ofertas.append(url_oferta)\n",
    "        \n",
    "    return url_ofertas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3778457-d734-48da-92ea-247b1ef13226",
   "metadata": {},
   "source": [
    "## 2.2. Información ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb6a2ae-7a96-4126-b543-2b4c469227ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraper_talenthacker(url_empleos, opts, bullet_point):\n",
    "    # Inicio browser:\n",
    "    browser = webdriver.Chrome(options=opts)\n",
    "    browser.maximize_window()\n",
    "    \n",
    "    # Valores fijos:\n",
    "    portal = \"talenthacker\"\n",
    "    fecha_scrapeo = datetime.datetime.now().date()\n",
    "    \n",
    "    \n",
    "    datos_ofertas_empleo = []\n",
    "    contador = 0\n",
    "    for empleo in url_empleos:\n",
    "        \n",
    "        browser.get(empleo)\n",
    "        sleep(2)\n",
    "        \n",
    "        try:\n",
    "            # Acepto cookies\n",
    "            browser.find_element(By.CLASS_NAME, \"q-banner__actions\").find_element(By.CLASS_NAME, \"buttons-wrapper\").find_element(By.CLASS_NAME, \"q-btn--standard\").click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        soup_oferta = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        \n",
    "        #########################  INFORMACION  ##########################################################################\n",
    "        \n",
    "        try:\n",
    "            titulo = soup_oferta.find(\"span\", class_= \"spot-title\").text\n",
    "        except:\n",
    "            titulo = np.nan\n",
    "            \n",
    "        try:\n",
    "            herramientas = [herramienta.text for herramienta in soup_oferta.find(\"div\", class_= \"spot-skills\").find_all(\"a\")]\n",
    "        except:\n",
    "            herramientas = np.nan\n",
    "            \n",
    "        try:\n",
    "            descripcion = soup_oferta.find_all(\"div\", class_= \"block full-width text-body2\")[0].text\n",
    "        except:\n",
    "            descripcion = np.nan\n",
    "            \n",
    "        try:\n",
    "            funciones = soup_oferta.find_all(\"div\", class_= \"block full-width text-body2\")[1].text\n",
    "        except:\n",
    "            funciones = np.nan\n",
    "            \n",
    "            \n",
    "        imagenes = soup_oferta.find_all(\"div\", class_= \"th-body-md flex no-wrap q-ma-sm\")\n",
    "        for imagen in imagenes:\n",
    "            img = imagen.find(\"img\")[\"src\"]\n",
    "            \n",
    "            if img == bullet_point[\"ubicacion\"]:\n",
    "                try:\n",
    "                    ubicacion = imagen.text\n",
    "                except:\n",
    "                    ubicacion = np.nan\n",
    "                    \n",
    "            elif img == bullet_point[\"presencialidad\"]:\n",
    "                try:\n",
    "                    presencialidad = imagen.text.strip()\n",
    "                except:\n",
    "                    presencialidad = np.nan\n",
    "                    \n",
    "            elif img == bullet_point[\"jornada_tipo\"]:\n",
    "                try:\n",
    "                    tipo_contrato = imagen.text.split(\"·\")[0].strip()\n",
    "                    jornada = imagen.text.split(\"·\")[1].strip()\n",
    "                except:\n",
    "                    tipo_contrato = np.nan \n",
    "                    jornada = np.nan\n",
    "            \n",
    "            if img == bullet_point[\"experiencia\"]:\n",
    "                try:\n",
    "                    experiencia = imagen.text.strip()\n",
    "                except:\n",
    "                    experiencia = np.nan\n",
    "\n",
    "            \n",
    "            elif img == bullet_point[\"salario\"]:\n",
    "                try:\n",
    "                    salario = imagen.text.strip()\n",
    "                except:\n",
    "                    salario = np.nan\n",
    "\n",
    "        \n",
    "        ###################################################################################################################\n",
    "        datos_ofertas_empleo.append([titulo, herramientas, descripcion, ubicacion, presencialidad, funciones, jornada, experiencia, tipo_contrato, salario, fecha_scrapeo, empleo, portal])\n",
    "        contador += 1\n",
    "\n",
    "        if contador % 100 == 0:\n",
    "            sleep(20)\n",
    "            \n",
    "            \n",
    "    df = pd.DataFrame(datos_ofertas_empleo, columns= [\"titulo\", \"herramientas\", \"descripcion\", \"ubicacion\", \"presencialidad\", \"funciones\", \"jornada\", \"experiencia\", \"tipo_contrato\", \"salario\", \"fecha_scrapeo\", \"url\", \"portal\"])\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21888a-a2c0-40b9-81e5-ceb41d6a57e5",
   "metadata": {},
   "source": [
    "# 3. Tecnoempleo Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c75a7-f4dd-47b6-8c8d-cafe48b16fbe",
   "metadata": {},
   "source": [
    "## 3.1. Número de páginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c60acf14-d4f3-48d1-bdfa-20283c9d5b98",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pag_counter(opts,limite=1000, nuevos= True):\n",
    "\n",
    "    # Inicializo navegador:\n",
    "    browser = webdriver.Chrome(options=opts)\n",
    "    browser.maximize_window()\n",
    "    \n",
    "    # Entro en tecnoempleo:\n",
    "    if nuevos:\n",
    "        url_paginas = f\"https://www.tecnoempleo.com/ofertas-trabajo/?ult_24h=,1,&pagina={limite}\"\n",
    "        \n",
    "    else:\n",
    "        url_paginas = f\"https://www.tecnoempleo.com/ofertas-trabajo/?pagina={limite}\"\n",
    "    \n",
    "    browser.get(url_paginas)\n",
    "    \n",
    "    # Acepto cookies\n",
    "    browser.find_elements(By.CLASS_NAME, \"col-6\")[0].click()\n",
    "    \n",
    "    # Saco el número de páginas\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    num_paginas = int(soup.find(\"li\", class_= \"active\").text)\n",
    "    \n",
    "    return num_paginas    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca40a9-48e2-433b-b3bf-847a7a917fe2",
   "metadata": {},
   "source": [
    "## 3.2. Información ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8932d7d9-107d-4d47-9e81-3412953b948e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraper_tecnoempleo(num_paginas, opts, nuevos= True):\n",
    "    \n",
    "    portal = \"tecnoempleo\"\n",
    "    fecha_scrapeo = datetime.datetime.now().date()\n",
    "    \n",
    "    # Inicializo navegador:\n",
    "    browser = webdriver.Chrome(options=opts)\n",
    "    browser.maximize_window()\n",
    "    \n",
    "    datos_ofertas_empleo = []\n",
    "    \n",
    "    contador = 0\n",
    "    for pagina in range(num_paginas + 1):\n",
    "        \n",
    "        if nuevos:\n",
    "            tecno_url = f\"https://www.tecnoempleo.com/ofertas-trabajo/?ult_24h=,1,&pagina={pagina}\"\n",
    "        else:\n",
    "            tecno_url = f\"https://www.tecnoempleo.com/ofertas-trabajo/?pagina={pagina}\"\n",
    "\n",
    "        browser.get(tecno_url)\n",
    "        sleep(1)\n",
    "        \n",
    "        # Acepto cookies\n",
    "        try:\n",
    "            browser.find_elements(By.CLASS_NAME, \"col-6\")[0].click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Saco las urls de cada puesto ofertado y saco la información\n",
    "        soup_page = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "        ofertas_empleo = soup_page.find_all(\"div\", class_= \"col-10\")\n",
    "\n",
    "        for oferta in ofertas_empleo:\n",
    "\n",
    "            # URL oferta de empleo\n",
    "            url_oferta = oferta.find(\"a\")[\"href\"]\n",
    "            # Entro en la oferta\n",
    "            browser.get(url_oferta)\n",
    "            sleep(1)\n",
    "            # Saco información relevante de la oferta\n",
    "            soup_oferta = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "            lista_li = soup_oferta.find(\"ul\", class_= \"fs--15\").find_all(\"li\", class_= \"border-bottom\")\n",
    "\n",
    "            ubicacion, funciones, jornada, experiencia, tipo_contrato, salario = [np.nan] * 6\n",
    "            for li in lista_li:\n",
    "\n",
    "                tag = li.find(\"span\", class_= \"d-inline-block\").text\n",
    "\n",
    "                if tag == \"Ubicación\" : \n",
    "                    ubicacion = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "                elif tag == \"Funciones\":\n",
    "                    funciones = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "                elif tag == \"Jornada\":\n",
    "                    jornada = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "                elif tag == \"Experiencia\":\n",
    "                    experiencia = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "                elif tag == \"Tipo contrato\":\n",
    "                    tipo_contrato = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "                elif tag == \"Salario\":\n",
    "                    salario = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "            titulo = soup_oferta.find(\"div\", class_= \"col-lg-8\").find(\"h1\").text.replace(\"\\t\", \"\").replace(\"Urgente\\n\", \"\").strip()\n",
    "        \n",
    "            try:\n",
    "                empresa = soup_oferta.find(\"div\", class_= \"col-lg-8\").find(\"a\", class_= \"fs--18\").text.strip()\n",
    "            except:\n",
    "                empresa = np.nan\n",
    "\n",
    "            fecha = soup_oferta.find(\"div\", class_= \"col-lg-8\").find(\"span\", \"ml-4\").text.strip().replace(\"Actualizada\", \"\")\n",
    "\n",
    "            herramientas = soup_oferta.find(\"ul\", class_= \"fs--15\").find(\"li\", class_= \"mb-3\").text.strip().replace(\"\\n\", \", \")\n",
    "\n",
    "            descripcion = soup_oferta.find(\"div\", class_= \"mt-4\").text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"Descripción de la oferta de empleo\", \"\").strip()\n",
    "\n",
    "            datos_ofertas_empleo.append([titulo, empresa, fecha, herramientas, descripcion, ubicacion, funciones, jornada, experiencia, tipo_contrato, salario, fecha_scrapeo, url_oferta, portal])\n",
    "            \n",
    "        contador =+ 1\n",
    "        if contador % 2 == 0:\n",
    "            sleep(3)\n",
    "            \n",
    "    df = pd.DataFrame(datos_ofertas_empleo, columns= [\"titulo\", \"empresa\", \"fecha\", \"herramientas\", \"descripcion\", \"ubicacion\", \"funciones\", \"jornada\", \"experiencia\", \"tipo_contrato\", \"salario\", \"fecha_scrapeo\", \"url\", \"portal\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad97192-21c6-498b-8a1f-c6b3fe8fc22f",
   "metadata": {},
   "source": [
    "# 4. Infojobs Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15b92a-95f5-407c-aab7-6c556d79e1bb",
   "metadata": {},
   "source": [
    "## 4.1. URL ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a6e9a61-661b-4d8d-bd4e-57fe6407ddc8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def url_scraper_infojobs(opts, api_key, nuevas= True):\n",
    "    pagina = 1\n",
    "    \n",
    "    if nuevas:\n",
    "        url_infojobs = f\"https://www.infojobs.net/jobsearch/search-results/list.xhtml?keyword=&categoryIds=150&segmentId=&page={pagina}&sortBy=PUBLICATION_DATE&onlyForeignCountry=false&sinceDate=_24_HOURS\"\n",
    "    \n",
    "    else:\n",
    "        url_infojobs = f\"https://www.infojobs.net/ofertas-trabajo/informatica-telecomunicaciones?keyword=&categoryIds=150&segmentId=&page={pagina}&sortBy=PUBLICATION_DATE&onlyForeignCountry=false&sinceDate=ANY\"\n",
    "    \n",
    "    # Initialize:\n",
    "    browser = webdriver.Chrome(options=opts)\n",
    "    browser.maximize_window()\n",
    "    browser.get(url_infojobs)\n",
    "    sleep(4)\n",
    "    browser.get(url_infojobs)\n",
    "    sleep(4)\n",
    "\n",
    "    ##################################################Captcha##################################################\n",
    "\n",
    "    browser.find_element(By.CLASS_NAME, \"geetest_radar_tip\").click()\n",
    "    sleep(2)\n",
    "    browser.find_element(By.CLASS_NAME, \"geetest_voice\").click()\n",
    "    sleep(2)\n",
    "    browser.find_element(By.CLASS_NAME, \"geetest_replay\").click()\n",
    "    audio_data = record_audio(duration=15)\n",
    "    sleep(2)\n",
    "    save_audio(audio_data)\n",
    "    sleep(2)\n",
    "\n",
    "    client = OpenAI(api_key= api_key)\n",
    "    audio_file = os.getcwd() + \"\\\\recorded_audio.mp3\"\n",
    "\n",
    "    audio_file= open(audio_file, \"rb\")\n",
    "    transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
    "\n",
    "    codigo = transcript.text.replace(\"Introduzca lo que oiga.\", \"\").replace(\", \", \"\").replace(\".\", \"\").strip()\n",
    "\n",
    "    barra = browser.find_element(By.CLASS_NAME, \"geetest_input\")\n",
    "    barra.send_keys(codigo)\n",
    "    sleep(2)\n",
    "    browser.find_element(By.CLASS_NAME, \"geetest_box\").find_element(By.CLASS_NAME, \"geetest_btn\").click()\n",
    "    sleep(5)\n",
    "    ###########################################################################################################\n",
    "\n",
    "    # Acepto cookies\n",
    "    browser.find_elements(By.ID, \"didomi-notice-agree-button\")[0].click()\n",
    "    sleep(1)\n",
    "    \n",
    "    url_empleos = []\n",
    "    while True:\n",
    "\n",
    "        # Saco url empleos:\n",
    "        soup_urls = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            urls = soup_urls.find_all(\"div\", class_= \"ij-OfferCardContent-description\")\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for url in urls:\n",
    "            oferta_url = url.find(\"h2\").find(\"a\")[\"href\"].replace(\"//\", \"\")\n",
    "            url_empleos.append(oferta_url)\n",
    "\n",
    "        pagina += 1\n",
    "        browser.find_elements(By.CLASS_NAME, \"sui-MoleculePagination-item\")[-1].click()\n",
    "        sleep(2)\n",
    "\n",
    "        lim_paginas = len(browser.find_elements(By.CLASS_NAME, \"sui-MoleculePagination-item\"))\n",
    "\n",
    "        if pagina > 1:\n",
    "            if lim_paginas < 7:\n",
    "                break\n",
    "                \n",
    "    url_empleos_final = [\"https://\" + empleo for empleo in url_empleos]\n",
    "    \n",
    "    return url_empleos_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8456dd-b564-4ab3-802f-965dcbf5edf7",
   "metadata": {},
   "source": [
    "## 4.2. Información ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd83051d-4a8f-4945-a07e-528649219bc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraper_infojobs(url_empleos_final, opts, api_key):\n",
    "    datos_ofertas_empleo = []\n",
    "    contador = 0\n",
    "    \n",
    "    portal = \"infojobs\"\n",
    "    fecha_scrapeo = datetime.datetime.now().date()\n",
    "\n",
    "    for empleo in url_empleos_final[contador:]:\n",
    "\n",
    "        if contador == 0:\n",
    "            browser = webdriver.Chrome(options=opts)\n",
    "            browser.maximize_window()\n",
    "            browser.get(empleo)\n",
    "            sleep(4)\n",
    "\n",
    "            ##########Captcha##########\n",
    "            browser.find_element(By.CLASS_NAME, \"geetest_radar_tip\").click()\n",
    "            sleep(2)\n",
    "            browser.find_element(By.CLASS_NAME, \"geetest_voice\").click()\n",
    "            sleep(2)\n",
    "            browser.find_element(By.CLASS_NAME, \"geetest_replay\").click()\n",
    "            audio_data = record_audio(duration=15)\n",
    "            sleep(2)\n",
    "            save_audio(audio_data)\n",
    "\n",
    "            client = OpenAI(api_key= api_key)\n",
    "            audio_file = os.getcwd() + \"\\\\recorded_audio.mp3\"\n",
    "\n",
    "            audio_file= open(audio_file, \"rb\")\n",
    "            transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
    "\n",
    "            codigo = transcript.text.replace(\"Introduzca lo que oiga.\", \"\").replace(\", \", \"\").replace(\".\", \"\").strip()\n",
    "\n",
    "            barra = browser.find_element(By.CLASS_NAME, \"geetest_input\")\n",
    "            barra.send_keys(codigo)\n",
    "            sleep(2)\n",
    "            browser.find_element(By.CLASS_NAME, \"geetest_box\").find_element(By.CLASS_NAME, \"geetest_btn\").click()\n",
    "            sleep(5)\n",
    "            ###########################\n",
    "\n",
    "            # Acepto cookies\n",
    "            browser.find_elements(By.ID, \"didomi-notice-agree-button\")[0].click()\n",
    "            sleep(1)\n",
    "\n",
    "        else:\n",
    "            browser.get(empleo)\n",
    "            sleep(3)\n",
    "\n",
    "        # Saco información relevante de la oferta:    \n",
    "        soup_oferta = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            captcha = soup_oferta.find(\"div\", attrs= attrs_ad).text        \n",
    "            if captcha == \"Hacer clic para comprobarReintentar\":\n",
    "                ##########Captcha##########\n",
    "                browser.find_element(By.CLASS_NAME, \"geetest_radar_tip\").click()\n",
    "                sleep(2)\n",
    "                browser.find_element(By.CLASS_NAME, \"geetest_voice\").click()\n",
    "                sleep(2)\n",
    "                browser.find_element(By.CLASS_NAME, \"geetest_replay\").click()\n",
    "                audio_data = record_audio(duration=15)\n",
    "                sleep(2)\n",
    "                save_audio(audio_data)\n",
    "\n",
    "                client = OpenAI(api_key= api_key)\n",
    "                audio_file = os.getcwd() + \"\\\\recorded_audio.mp3\"\n",
    "\n",
    "                audio_file= open(audio_file, \"rb\")\n",
    "                transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
    "\n",
    "                codigo = transcript.text.replace(\"Introduzca lo que oiga.\", \"\").replace(\", \", \"\").replace(\".\", \"\").strip()\n",
    "\n",
    "                barra = browser.find_element(By.CLASS_NAME, \"geetest_input\")\n",
    "                barra.send_keys(codigo)\n",
    "                sleep(2)\n",
    "                browser.find_element(By.CLASS_NAME, \"geetest_box\").find_element(By.CLASS_NAME, \"geetest_btn\").click()\n",
    "                sleep(5)\n",
    "                ###########################\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            attrs_tit= {\"id\": \"prefijoPuesto\"}\n",
    "            titulo = soup_oferta.find(\"div\", class_= \"heading-addons\").find(\"h1\", attrs= attrs_tit).text\n",
    "        except:\n",
    "            titulo = np.nan\n",
    "        try:\n",
    "            empresa = soup_oferta.find(\"div\", class_= \"heading-addons\").find(\"a\", class_= \"link\").text\n",
    "        except:\n",
    "            empresa = np.nan\n",
    "        try:\n",
    "            fecha = soup_oferta.find(\"div\", class_= \"row-matrioska\").find(\"span\", class_= \"marked\").text\n",
    "        except:\n",
    "            try:\n",
    "                lista_bullets = soup_oferta.find(\"div\", class_= \"row-matrioska\").find(\"ul\", class_= \"list-bullet-default\").find_all(\"li\")\n",
    "                for bullet in lista_bullets:\n",
    "                    tag_bullet = bullet.text.strip()[:9]\n",
    "                    if tag_bullet == \"Publicada\":\n",
    "                        fecha = bullet.text.strip()[12:]\n",
    "            except:\n",
    "                fecha = np.nan\n",
    "\n",
    "        attrs_ubic= {\"id\": \"prefijoPoblacion\"}\n",
    "        try:\n",
    "            ubicacion = soup_oferta.find(\"div\", class_= \"row-matrioska\").find(\"span\", attrs= attrs_ubic).text.replace(\",\", \"\").strip()\n",
    "        except:\n",
    "            ubicacion = np.nan\n",
    "\n",
    "        attrs_descr= {\"id\": \"prefijoDescripcion1\"}\n",
    "        try:\n",
    "            descripcion = soup_oferta.find(\"div\", attrs= attrs_descr).text.replace(\"\\n\", \"\").strip()\n",
    "        except:\n",
    "            descripcion = np.nan\n",
    "        try:\n",
    "            lista_li = soup_oferta.find(\"div\", class_= \"row-matrioska\").find_all(\"li\")        \n",
    "\n",
    "            for li in lista_li:\n",
    "\n",
    "                tag = li.find(\"span\").text[:7]\n",
    "\n",
    "                if tag == \"Salario\":\n",
    "                    salario = li.find(\"span\").text[8:]\n",
    "\n",
    "                elif tag == \"Experie\":\n",
    "                    experiencia = li.find(\"span\").text[20:]\n",
    "\n",
    "                elif tag == \"Tipo de\":\n",
    "\n",
    "                    tipo_contrato = li.find(\"span\").text[18:].split(\",\")[0]\n",
    "\n",
    "                    jornada = li.find(\"span\").text[18:].split(\",\")[-1]\n",
    "        except:\n",
    "            salario = np.nan\n",
    "            experiencia = np.nan\n",
    "            tipo_contrato = np.nan\n",
    "            jornada = np.nan\n",
    "\n",
    "        try:\n",
    "            lista_herramientas = soup_oferta.find(\"div\", class_= \"inner-expanded\").find(\"ul\", class_= \"list-default\").find_all(\"a\")\n",
    "            herramientas = [herramienta.text for herramienta in lista_herramientas]\n",
    "        except:\n",
    "            herramientas = np.nan\n",
    "\n",
    "        try:\n",
    "            funciones = soup_oferta.find(\"div\", class_= \"border-top\").find(\"ul\").find(\"span\", class_= \"list-default-text\").text\n",
    "        except:\n",
    "            funciones = np.nan\n",
    "\n",
    "        datos_ofertas_empleo.append([titulo, empresa, fecha, herramientas, descripcion, ubicacion, funciones, jornada, experiencia, tipo_contrato, salario, fecha_scrapeo, empleo, portal])\n",
    "        contador += 1\n",
    "\n",
    "        if contador % 100 == 0:\n",
    "            sleep(20)\n",
    "            \n",
    "    df_infojobs = pd.DataFrame(datos_ofertas_empleo, columns= [\"titulo\", \"empresa\", \"fecha\", \"herramientas\", \"descripcion\", \"ubicacion\", \"funciones\", \"jornada\", \"experiencia\", \"tipo_contrato\", \"salario\", \"fecha_scrapeo\", \"url\", \"portal\"])\n",
    "    df_infojobs = df_infojobs.dropna(how= \"all\")\n",
    "    \n",
    "    return df_infojobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e1cc6-c7b7-4ba4-ba8f-ac943d8880a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
