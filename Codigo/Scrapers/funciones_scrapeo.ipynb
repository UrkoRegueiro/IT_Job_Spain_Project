{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714c1be9-3356-4127-8fe1-639f8846c617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Básicas:\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Web Scraping:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Captcha sol:\n",
    "from openai import OpenAI\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c844698-169d-4c9b-99ec-330750d19c92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Funciones Especiales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5ab45-4841-4b91-99bc-4b391b2c62b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce71811-09b1-472a-852f-756cbbdd8c53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def record_audio(duration=15, sample_rate=44100):\n",
    "    print(\"Recording...\")\n",
    "\n",
    "    # Record audio\n",
    "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=2, dtype=np.int16)\n",
    "    sd.wait()\n",
    "\n",
    "    print(\"Recording finished.\")\n",
    "\n",
    "    return audio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3853cb-c3cb-4a53-8cca-42109506c653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_audio(audio_data, file_path=\"recorded_audio.mp3\", sample_rate=44100):\n",
    "    print(\"Saving audio...\")\n",
    "\n",
    "    # Save audio to file using scipy.io.wavfile.write\n",
    "    write(file_path, sample_rate, audio_data)\n",
    "\n",
    "    print(f\"Audio saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29bc0f-659f-48f3-8c29-d81b8f4e42c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extracción de datos de tipo texto con try except"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5316c419-fa2f-4996-8120-41080eab4b39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extraccion_datos_try_except_compleja(soup, tags_attrs, elementos_a_encontrar = 'lista_completa'): # Si pasamos el parametro lista completa cogeremos todos los elementos \n",
    "                                                                                    #que encuetre con find_all\n",
    "    \n",
    "    #Hacemos todas las busquedas simples necesrias con el metodo .find y si falla devolvemos tantos nans como elementos buscabamos\n",
    "    for tag, attrs in tags_attrs[:-1]:\n",
    "        try:\n",
    "            soup = soup.find(tag, attrs)\n",
    "        except:\n",
    "            datos = np.nan if elementos_a_encontrar == 'lista_completa' else [np.nan] * len(elementos_a_encontrar)\n",
    "\n",
    "            return datos\n",
    "\n",
    "    #Realizamos la busqueda con el find_all\n",
    "    datos = []\n",
    "    tag, attrs = tags_attrs[-1]\n",
    "    \n",
    "    if elementos_a_encontrar == 'lista_completa':\n",
    "        try:\n",
    "            datos = soup.find_all(tag, attrs)\n",
    "            datos = [dato.get_text(strip = True) for dato in datos]\n",
    "        except:\n",
    "            datos = np.nan\n",
    "        \n",
    "        return datos\n",
    "        \n",
    "    for elemento in elementos_a_encontrar:\n",
    "        try:\n",
    "            texto = soup.find_all(tag, attrs)[elemento].get_text(strip = True)\n",
    "        except:\n",
    "            texto = np.nan\n",
    "        \n",
    "        datos.append(texto)\n",
    "\n",
    "    return datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b25513-30b3-4640-b17d-241c41b2dd5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extraccion_datos_try_except_simple(soup, tags_attrs):\n",
    "    try:\n",
    "        for idx, (tag, attrs) in enumerate(tags_attrs):\n",
    "            dato = soup.find(tag, attrs) if idx == 0 else dato.find(tag, attrs)\n",
    "        return dato.get_text(strip = True)\n",
    "    \n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3df27d10-8e14-4bbd-acd1-5d62e9f70f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraccion_datos_try_except(soup, tag, attrs):\n",
    "    try:\n",
    "        dato = soup.find(tag, attrs).get_text(strip = True)\n",
    "        return dato\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558f515-d754-4810-9f93-b82ddee56ae0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Guardado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62159f5-ea81-4427-898c-2306f83d98fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def guardar_datos(portal, data, nombres_columnas, ruta_datos):\n",
    "    try:\n",
    "        df = pd.read_csv(ruta_datos + f'datos_{portal}.csv')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        df = pd.DataFrame(columns=nombres_columnas)\n",
    "    \n",
    "    nuevas_filas = pd.DataFrame(data, columns=nombres_columnas)\n",
    "    \n",
    "    df = pd.concat([df, nuevas_filas], ignore_index=True) \n",
    "    \n",
    "    df.to_csv(ruta_datos + f'datos_{portal}.csv', index=False, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8b46f7-236d-48ca-8d42-41d6c1efd8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def date_matching(date):\n",
    "    match = re.search(r'(\\d{1,2})/(\\d{1,2})/(\\d{4})|(\\d+)\\s*(minutos|hora|dia|semana|mes)', date)\n",
    "    \n",
    "    if match:\n",
    "        if match.group(1):\n",
    "            day = int(match.group(1))\n",
    "            month = int(match.group(2))\n",
    "            year = int(match.group(3))\n",
    "            return relativedelta(day=day, month=month, year=year)\n",
    "        else:\n",
    "            cantidad = int(match.group(4))\n",
    "            unidad = match.group(5)\n",
    "        \n",
    "            if unidad == 'minutos':\n",
    "                return relativedelta(minutes=cantidad)\n",
    "            elif unidad == 'hora':\n",
    "                return relativedelta(hours=cantidad)\n",
    "            elif unidad == 'dia':\n",
    "                return relativedelta(days=cantidad)\n",
    "            elif unidad == 'semana':\n",
    "                return relativedelta(weeks=cantidad)\n",
    "            elif unidad == 'mes':\n",
    "                return relativedelta(months=cantidad)\n",
    "    \n",
    "    return relativedelta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "172ff92c-a651-4fb6-a654-1c12ad1cc021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def procesar_datos_nuevos(df, portal, ruta_datos):\n",
    "    #Pasamos todos los datos de fechas a cadena de texto para poder procesarlos\n",
    "    df['fecha_scrapeo'] = df['fecha_scrapeo'].apply(lambda x : str(x))\n",
    "\n",
    "    #Datetime\n",
    "    df['fecha_scrapeo'] = pd.to_datetime(df['fecha_scrapeo'], format='%Y-%m-%d', errors='coerce').dt.date\n",
    "\n",
    "    #Separamos las ofertas por las scrapeadas recientemente y las antiguas\n",
    "    ultima_fecha = df['fecha_scrapeo'].unique().max()\n",
    "    datos_nuevos = df[df['fecha_scrapeo'] == ultima_fecha]\n",
    "    datos_antiguos = df.drop(datos_nuevos.index)\n",
    "\n",
    "    #Si existe alguna oferta repetida, eliminamos el registro de la mas antigua\n",
    "    try:\n",
    "        merged_df = pd.merge(datos_nuevos, datos_antiguos, how = 'outer', on = ['titulo', 'empresa', 'descripcion'], indicator = True)\n",
    "    \n",
    "    except:\n",
    "        merged_df = pd.merge(datos_nuevos, datos_antiguos, how = 'outer', on = ['titulo', 'descripcion'], indicator = True)\n",
    "        \n",
    "    datos_nuevos = merged_df[merged_df['_merge'] == 'left_only'].drop('_merge', axis = 1)\n",
    "\n",
    "    #Eliminamos las columnas mergeadas del dataset antiguo y renombramos las de los datos nuevos\n",
    "    columnas_eliminar = [columna for columna in datos_nuevos.columns if columna.endswith('_y')]\n",
    "    datos_nuevos = datos_nuevos.drop(columnas_eliminar, axis = 1)\n",
    "\n",
    "    datos_nuevos.columns = [columna[:-2] if columna.endswith('_x') else columna for columna in datos_nuevos.columns]\n",
    "\n",
    "    #Concatenamos los nuevos datos con los antiguos de cada portal y los guardamos\n",
    "    df_completo = pd.concat([datos_antiguos, datos_nuevos], ignore_index = True)\n",
    "\n",
    "    df_completo.to_csv(ruta_datos + f'datos_{portal}.csv', index = False)\n",
    "\n",
    "    #Devolvemos los nuevos datos para aplicarles las funciones de limpieza\n",
    "    datos_nuevos.to_csv(ruta_datos + f'datos_nuevos_{portal}_{ultima_fecha}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a650ee-c3a9-4738-a93b-01d3c9e32623",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Infoempleo Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e5c48-3497-4b17-9e73-7fab4eb6e991",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. URL ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cbfcdae-ae06-48c4-9385-e950fb8876f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def url_scraper_infoempleo(driver, dic_datos):\n",
    "    nuevas = dic_datos['opcion']\n",
    "\n",
    "    # Acepto cookies\n",
    "    driver.find_element(By.ID, \"onetrust-accept-btn-handler\").click()\n",
    "    sleep(1)\n",
    "    \n",
    "    url_empleos = []\n",
    "    while True:\n",
    "\n",
    "        try:\n",
    "            driver.find_element(By.ID, \"lightbox\").find_element(By.CLASS_NAME, \"close\").click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if nuevas:\n",
    "            # Boton ofertas en las últimos 15 dias:\n",
    "            driver.find_element(By.ID, nuevas).click()\n",
    "            nuevas = False\n",
    "            sleep(1)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        empleos = soup.find(\"div\", class_= \"main-content\").find_all(\"li\", class_= \"offerblock\")\n",
    "\n",
    "        for url in empleos:\n",
    "            try:\n",
    "                url_oferta = \"https://www.infoempleo.com\" + url.find(\"a\")[\"href\"]\n",
    "                url_empleos.append(url_oferta)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            # Hago scroll hasta el final:\n",
    "            elemento_objetivo = driver.find_elements(By.CLASS_NAME, \"related-offer-item\")[-1]\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", elemento_objetivo)\n",
    "            sleep(1)\n",
    "            # Siguiente página:\n",
    "            driver.find_element(By.CLASS_NAME, \"pagination\").find_element(By.CLASS_NAME, \"next\").click()\n",
    "            sleep(2)\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    return url_empleos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103c21aa-87d9-4f0c-bc6f-2c9beb9a9640",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2. Información ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9623d53a-9467-4c90-9413-5b3de742e4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraper_infoempleo(driver, fecha_scrapeo, portal, nombres_columnas, ruta_datos, dic_datos):\n",
    "    url_empleos = url_scraper_infoempleo(driver = driver, dic_datos = dic_datos)\n",
    "    \n",
    "    data = []\n",
    "    contador = 0\n",
    "    for empleo in url_empleos:\n",
    "    \n",
    "        driver.get(empleo)\n",
    "        sleep(2)\n",
    "\n",
    "        try:\n",
    "            # Acepto cookies\n",
    "            driver.find_element(By.ID, \"onetrust-accept-btn-handler\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Cierro popup si aparece:\n",
    "        try:\n",
    "            driver.find_element(By.ID, \"lightbox\").find_element(By.CLASS_NAME, \"close\").click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        titulo = extraccion_datos_try_except_simple(soup, [(\"div\", {'class' : \"title-wrapper\"}), ('h1',{})])\n",
    "\n",
    "        empresa = extraccion_datos_try_except_simple(soup, [(\"div\", {'class' : \"title-wrapper\"}), ('li',{'class' : 'companyname'})])\n",
    "\n",
    "        presencialidad = extraccion_datos_try_except_simple(soup, [(\"div\", {'class' : \"title-wrapper\"}), ('li',{'class' : 'badge'})])\n",
    "\n",
    "        fecha = extraccion_datos_try_except_simple(soup, [(\"div\", {'class' : \"title-wrapper\"}), ('li',{'class' : 'mt10'})])\n",
    "\n",
    "        ubicacion = extraccion_datos_try_except_simple(soup, [(\"div\", {'class' : \"title-wrapper\"}), ('li',{'class' : 'block'})])\n",
    "\n",
    "        # Bloque de características:\n",
    "\n",
    "        try:\n",
    "            driver.find_element(By.CLASS_NAME, \"areapos-vmore\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        bullet_points = soup.find(\"div\", class_= \"offer-excerpt\").find_all(\"ul\", class_= \"inline\")\n",
    "\n",
    "        try:\n",
    "            experiencia = bullet_points[0].find_all(\"p\")[0].text\n",
    "        except:\n",
    "            experiencia = np.nan\n",
    "\n",
    "        try:\n",
    "            salario = bullet_points[0].find_all(\"p\")[1].text\n",
    "        except:\n",
    "            salario = np.nan\n",
    "\n",
    "        try:\n",
    "            funciones = [funcion.text for funcion in bullet_points[1].find_all(\"li\")[1:]]\n",
    "        except:\n",
    "            funciones = np.nan\n",
    "\n",
    "        try:\n",
    "            solicitudes = bullet_points[2].find_all(\"p\")[1].text\n",
    "        except:\n",
    "            solicitudes = np.nan        \n",
    "\n",
    "        try:\n",
    "            tipo_contrato = bullet_points[3].find_all(\"p\")[0].text\n",
    "        except:\n",
    "            tipo_contrato = np.nan\n",
    "\n",
    "        try:\n",
    "            jornada = bullet_points[3].find_all(\"p\")[1].text\n",
    "        except:\n",
    "            jornada = np.nan  \n",
    "\n",
    "        cuerpo_oferta = soup.find(\"div\", class_= \"offer\").find_all(\"pre\")\n",
    "\n",
    "        try:\n",
    "            descripcion = cuerpo_oferta[0].text.replace(\"\\n\", \"\").replace(\"*\", \"\").strip()\n",
    "        except:\n",
    "            descripcion = np.nan    \n",
    "\n",
    "        try:\n",
    "            herramientas = cuerpo_oferta[1].text.replace(\"\\n\", \"\").replace(\"*\", \"\").strip()\n",
    "        except:\n",
    "            herramientas = np.nan\n",
    "\n",
    "        data.append([titulo, empresa, fecha, herramientas, descripcion, ubicacion, presencialidad, funciones, jornada, experiencia, tipo_contrato, salario, solicitudes, fecha_scrapeo, empleo, portal])\n",
    "        contador += 1\n",
    "\n",
    "        if contador % 100 == 0:\n",
    "            sleep(20)\n",
    "            \n",
    "    guardar_datos(portal = portal, data = data, nombres_columnas = nombres_columnas, ruta_datos = ruta_datos)\n",
    "    print(f'{portal} - Scrapeo completado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5735c1f-1415-48c5-a466-e819fba926b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2. Talenthacker Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe7601-528e-41b7-9a9d-ba0f851bad11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.1. URL ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0fef6ef-6817-4772-969d-8689863e460f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def url_scraper_talenthacker(driver, dic_datos = None):\n",
    "\n",
    "    # Acepto cookies:\n",
    "    driver.find_element(By.CLASS_NAME, \"q-banner__actions\").find_element(By.CLASS_NAME, \"buttons-wrapper\").find_element(By.CLASS_NAME, \"q-btn--standard\").click()\n",
    "\n",
    "    # Hago scroll hasta poder ver todas las ofertas disponibles:\n",
    "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    while True:\n",
    "        # Scroll:\n",
    "        driver.execute_script(\"window.scrollTo(0,document.documentElement.scrollHeight);\")\n",
    "        sleep(3)\n",
    "        # Calcula la altura del nuevo scroll y la compara con la del último:\n",
    "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            print(\"Ya no hay más página.\")\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    talent_hacker = \"https://talenthackers.net\"\n",
    "    ofertas = soup.find(\"div\", class_= \"jobs-list-wrapper\").find_all(\"div\", class_= \"col-12\")\n",
    "\n",
    "    url_ofertas = []\n",
    "    for oferta in ofertas:\n",
    "        url_oferta = talent_hacker + oferta.find(\"a\")[\"href\"]\n",
    "        url_ofertas.append(url_oferta)\n",
    "        \n",
    "    return url_ofertas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e97fed-7aa8-4417-9a25-2d4cadca917d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.2. Información ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0cfeac3-6f75-4207-84ae-e3d01f40ff6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraper_talenthacker(driver, fecha_scrapeo, portal, nombres_columnas, ruta_datos, dic_datos):\n",
    "    url_empleos = url_scraper_talenthacker(driver = driver, dic_datos = dic_datos)\n",
    "    \n",
    "    bullet_point = dic_datos['bullet_point']\n",
    "    \n",
    "    data = []\n",
    "    contador = 0\n",
    "    for empleo in url_empleos:\n",
    "        \n",
    "        driver.get(empleo)\n",
    "        sleep(2)\n",
    "        \n",
    "        try:\n",
    "            # Acepto cookies\n",
    "            driver.find_element(By.CLASS_NAME, \"q-banner__actions\").find_element(By.CLASS_NAME, \"buttons-wrapper\").find_element(By.CLASS_NAME, \"q-btn--standard\").click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        #########################  INFORMACION  ##########################################################################\n",
    "        \n",
    "        try:\n",
    "            titulo = soup.find(\"span\", class_= \"spot-title\").text\n",
    "        except:\n",
    "            titulo = np.nan\n",
    "            \n",
    "        try:\n",
    "            herramientas = [herramienta.text for herramienta in soup.find(\"div\", class_= \"spot-skills\").find_all(\"a\")]\n",
    "        except:\n",
    "            herramientas = np.nan\n",
    "            \n",
    "        try:\n",
    "            descripcion = soup.find_all(\"div\", class_= \"block full-width text-body2\")[0].text\n",
    "        except:\n",
    "            descripcion = np.nan\n",
    "            \n",
    "        try:\n",
    "            funciones = soup.find_all(\"div\", class_= \"block full-width text-body2\")[1].text\n",
    "        except:\n",
    "            funciones = np.nan\n",
    "            \n",
    "            \n",
    "        imagenes = soup.find_all(\"div\", class_= \"th-body-md flex no-wrap q-ma-sm\")\n",
    "        for imagen in imagenes:\n",
    "            img = imagen.find(\"img\")[\"src\"]\n",
    "            \n",
    "            if img == bullet_point[\"ubicacion\"]:\n",
    "                try:\n",
    "                    ubicacion = imagen.text\n",
    "                except:\n",
    "                    ubicacion = np.nan\n",
    "                    \n",
    "            elif img == bullet_point[\"presencialidad\"]:\n",
    "                try:\n",
    "                    presencialidad = imagen.text.strip()\n",
    "                except:\n",
    "                    presencialidad = np.nan\n",
    "                    \n",
    "            elif img == bullet_point[\"jornada_tipo\"]:\n",
    "                try:\n",
    "                    tipo_contrato = imagen.text.split(\"·\")[0].strip()\n",
    "                    jornada = imagen.text.split(\"·\")[1].strip()\n",
    "                except:\n",
    "                    tipo_contrato = np.nan \n",
    "                    jornada = np.nan\n",
    "            \n",
    "            if img == bullet_point[\"experiencia\"]:\n",
    "                try:\n",
    "                    experiencia = imagen.text.strip()\n",
    "                except:\n",
    "                    experiencia = np.nan\n",
    "\n",
    "            \n",
    "            elif img == bullet_point[\"salario\"]:\n",
    "                try:\n",
    "                    salario = imagen.text.strip()\n",
    "                except:\n",
    "                    salario = np.nan\n",
    "\n",
    "        \n",
    "        ###################################################################################################################\n",
    "        data.append([titulo, herramientas, descripcion, ubicacion, presencialidad, funciones, jornada, experiencia, tipo_contrato, salario, fecha_scrapeo, empleo, portal])\n",
    "        contador += 1\n",
    "\n",
    "        if contador % 100 == 0:\n",
    "            sleep(20)\n",
    "            \n",
    "            \n",
    "    guardar_datos(portal = portal, data = data, nombres_columnas = nombres_columnas, ruta_datos = ruta_datos)\n",
    "    print(f'{portal} - Scrapeo completado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f9773-96ad-4b5a-b8a2-085ad01d7517",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3. Tecnoempleo Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc14c4-8ce0-4973-aa48-ceb6fe6137ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1. Número de páginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e0d9aad-a55e-4983-b38e-c8cc4f3f6cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pag_counter(driver, dic_datos):\n",
    "    limite = dic_datos['limite']\n",
    "    nuevas = dic_datos['nuevas']\n",
    "    \n",
    "    # Entro en tecnoempleo:\n",
    "    if nuevas:\n",
    "        url_paginas = f\"https://www.tecnoempleo.com/ofertas-trabajo/?pagina=50\"\n",
    "        \n",
    "    else:\n",
    "        url_paginas = f\"https://www.tecnoempleo.com/ofertas-trabajo/?pagina={limite}\"\n",
    "    \n",
    "    driver.get(url_paginas)\n",
    "    \n",
    "    # Acepto cookies\n",
    "    driver.find_elements(By.CLASS_NAME, \"col-6\")[0].click()\n",
    "    \n",
    "    # Saco el número de páginas\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    num_paginas = int(soup.find(\"li\", class_= \"active\").text)\n",
    "    \n",
    "    return num_paginas    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb4fad-4b90-4e70-bf34-85c78e2c4998",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2. Información ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4326e216-869d-4cee-8f9a-b71dcab78add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraper_tecnoempleo(driver, fecha_scrapeo, portal, nombres_columnas, ruta_datos, dic_datos):\n",
    "    num_paginas = pag_counter(driver, dic_datos)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    contador = 0\n",
    "    for pagina in range(num_paginas + 1):\n",
    "        \n",
    "        tecno_url = f\"https://www.tecnoempleo.com/ofertas-trabajo/?pagina={pagina}\"\n",
    "\n",
    "        driver.get(tecno_url)\n",
    "        sleep(1)\n",
    "        \n",
    "        # Acepto cookies\n",
    "        try:\n",
    "            driver.find_elements(By.CLASS_NAME, \"col-6\")[0].click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Saco las urls de cada puesto ofertado y saco la información\n",
    "        soup_page = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        ofertas_empleo = soup_page.find_all(\"div\", class_= \"col-10\")\n",
    "\n",
    "        for oferta in ofertas_empleo:\n",
    "\n",
    "            # URL oferta de empleo\n",
    "            url_oferta = oferta.find(\"a\")[\"href\"]\n",
    "            # Entro en la oferta\n",
    "            driver.get(url_oferta)\n",
    "            sleep(1)\n",
    "            # Saco información relevante de la oferta\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            lista_li = soup.find(\"ul\", class_= \"fs--15\").find_all(\"li\", class_= \"border-bottom\")\n",
    "\n",
    "            ubicacion, funciones, jornada, experiencia, tipo_contrato, salario = [np.nan] * 6\n",
    "            for li in lista_li:\n",
    "\n",
    "                tag = li.find(\"span\", class_= \"d-inline-block\").text\n",
    "\n",
    "                if tag == \"Ubicación\" : \n",
    "                    ubicacion = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "                elif tag == \"Funciones\":\n",
    "                    funciones = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "                elif tag == \"Jornada\":\n",
    "                    jornada = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "                elif tag == \"Experiencia\":\n",
    "                    experiencia = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "                elif tag == \"Tipo contrato\":\n",
    "                    tipo_contrato = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "                elif tag == \"Salario\":\n",
    "                    salario = li.find(\"span\", class_= \"float-end\").text.strip().replace(\"\\xa0\", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "            titulo = soup.find(\"div\", class_= \"col-lg-8\").find(\"h1\").text.replace(\"\\t\", \"\").replace(\"Urgente\\n\", \"\").strip()\n",
    "        \n",
    "            try:\n",
    "                empresa = soup.find(\"div\", class_= \"col-lg-8\").find(\"a\", class_= \"fs--18\").text.strip()\n",
    "            except:\n",
    "                empresa = np.nan\n",
    "\n",
    "            fecha = soup.find(\"div\", class_= \"col-lg-8\").find(\"span\", \"ml-4\").text.strip().replace(\"Actualizada\", \"\")\n",
    "\n",
    "            herramientas = soup.find(\"ul\", class_= \"fs--15\").find(\"li\", class_= \"mb-3\").text.strip().replace(\"\\n\", \", \")\n",
    "\n",
    "            descripcion = soup.find(\"div\", class_= \"mt-4\").text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"Descripción de la oferta de empleo\", \"\").strip()\n",
    "\n",
    "            data.append([titulo, empresa, fecha, herramientas, descripcion, ubicacion, funciones, jornada, experiencia, tipo_contrato, salario, fecha_scrapeo, url_oferta, portal])\n",
    "            \n",
    "        contador =+ 1\n",
    "        if contador % 2 == 0:\n",
    "            sleep(3)\n",
    "            \n",
    "    guardar_datos(portal = portal, data = data, nombres_columnas = nombres_columnas, ruta_datos = ruta_datos)\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778422eb-b513-4f48-9a82-9bcfae419e30",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Infojobs Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05c081-69f7-4bba-ab72-5ca754564f98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.1. URL ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b32fb5b9-f3d8-4367-91ca-e3c1192073ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def url_scraper_infojobs(driver, dic_datos):\n",
    "    opcion = dic_datos['opcion']\n",
    "    api_key = dic_datos[\"api_key\"]\n",
    "    \n",
    "    pagina = 1\n",
    "    \n",
    "    url_infojobs = f\"https://www.infojobs.net/jobsearch/search-results/list.xhtml?keyword=&categoryIds=150&segmentId=&page={pagina}&sortBy=PUBLICATION_DATE&onlyForeignCountry=false&sinceDate={opcion}\"\n",
    "    \n",
    "    # Initialize:\n",
    "    driver.get(url_infojobs)\n",
    "    sleep(4)\n",
    "    driver.get(url_infojobs)\n",
    "    sleep(4)\n",
    "    \n",
    "    ##################################################Captcha##################################################\n",
    "\n",
    "    driver.find_element(By.CLASS_NAME, \"geetest_radar_tip\").click()\n",
    "    sleep(5)\n",
    "    driver.find_element(By.CLASS_NAME, \"geetest_voice\").click()\n",
    "    sleep(5)\n",
    "    driver.find_element(By.CLASS_NAME, \"geetest_replay\").click()\n",
    "    audio_data = record_audio()\n",
    "    sleep(2)\n",
    "    save_audio(audio_data)\n",
    "    sleep(2)\n",
    "\n",
    "    client = OpenAI(api_key= api_key)\n",
    "    audio_file = os.getcwd() + \"\\\\recorded_audio.mp3\"\n",
    "\n",
    "    audio_file= open(audio_file, \"rb\")\n",
    "    transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
    "\n",
    "    codigo = transcript.text.replace(\"Introduzca lo que oiga.\", \"\").replace(\", \", \"\").replace(\".\", \"\").strip()\n",
    "\n",
    "    barra = driver.find_element(By.CLASS_NAME, \"geetest_input\")\n",
    "    barra.send_keys(codigo)\n",
    "    sleep(2)\n",
    "    driver.find_element(By.CLASS_NAME, \"geetest_box\").find_element(By.CLASS_NAME, \"geetest_btn\").click()\n",
    "    sleep(5)\n",
    "    ###########################################################################################################\n",
    "\n",
    "    # Acepto cookies\n",
    "    driver.find_elements(By.ID, \"didomi-notice-agree-button\")[0].click()\n",
    "    sleep(1)\n",
    "    \n",
    "    url_empleos = []\n",
    "    while True:\n",
    "\n",
    "        # Saco url empleos:\n",
    "        soup_urls = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            urls = soup_urls.find_all(\"div\", class_= \"ij-OfferCardContent-description\")\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for url in urls:\n",
    "            oferta_url = url.find(\"h2\").find(\"a\")[\"href\"].replace(\"//\", \"\")\n",
    "            url_empleos.append(oferta_url)\n",
    "\n",
    "        pagina += 1\n",
    "        driver.find_elements(By.CLASS_NAME, \"sui-MoleculePagination-item\")[-1].click()\n",
    "        sleep(2)\n",
    "\n",
    "        lim_paginas = len(driver.find_elements(By.CLASS_NAME, \"sui-MoleculePagination-item\"))\n",
    "\n",
    "        if pagina > 1:\n",
    "            if lim_paginas < 7:\n",
    "                break\n",
    "                \n",
    "    url_empleos_final = [\"https://\" + empleo for empleo in url_empleos]\n",
    "    \n",
    "    return url_empleos_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6079e56d-1787-4ef7-b277-274d4d589c4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.2. Información ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ed5ca13-c48f-4890-9a96-0a5e0ae6aa5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraper_infojobs(driver, fecha_scrapeo, portal, nombres_columnas, ruta_datos, dic_datos):\n",
    "    url_empleos = url_scraper_infojobs(driver = driver, dic_datos = dic_datos)\n",
    "    api_key = dic_datos['api_key']\n",
    "    \n",
    "    data = []\n",
    "    contador = 0\n",
    "    \n",
    "    for empleo in url_empleos:\n",
    "\n",
    "        driver.get(empleo)\n",
    "        sleep(4)\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            ##########Captcha##########\n",
    "            driver.find_element(By.CLASS_NAME, \"geetest_radar_tip\").click()\n",
    "            sleep(2)\n",
    "            driver.find_element(By.CLASS_NAME, \"geetest_voice\").click()\n",
    "            sleep(2)\n",
    "            driver.find_element(By.CLASS_NAME, \"geetest_replay\").click()\n",
    "            audio_data = record_audio(duration=15)\n",
    "            sleep(2)\n",
    "            save_audio(audio_data)\n",
    "\n",
    "            client = OpenAI(api_key= api_key)\n",
    "            audio_file = os.getcwd() + \"\\\\recorded_audio.mp3\"\n",
    "\n",
    "            audio_file= open(audio_file, \"rb\")\n",
    "            transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
    "\n",
    "            codigo = transcript.text.replace(\"Introduzca lo que oiga.\", \"\").replace(\", \", \"\").replace(\".\", \"\").strip()\n",
    "\n",
    "            barra = driver.find_element(By.CLASS_NAME, \"geetest_input\")\n",
    "            barra.send_keys(codigo)\n",
    "            sleep(2)\n",
    "            driver.find_element(By.CLASS_NAME, \"geetest_box\").find_element(By.CLASS_NAME, \"geetest_btn\").click()\n",
    "            sleep(5)\n",
    "            ###########################\n",
    "\n",
    "            # Acepto cookies\n",
    "            driver.find_elements(By.ID, \"didomi-notice-agree-button\")[0].click()\n",
    "            sleep(1)\n",
    "\n",
    "        except:\n",
    "            next\n",
    "            \n",
    "        # Saco información relevante de la oferta:    \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            captcha = soup.find(\"div\", attrs= attrs_ad).text        \n",
    "            if captcha == \"Hacer clic para comprobarReintentar\":\n",
    "                ##########Captcha##########\n",
    "                driver.find_element(By.CLASS_NAME, \"geetest_radar_tip\").click()\n",
    "                sleep(2)\n",
    "                driver.find_element(By.CLASS_NAME, \"geetest_voice\").click()\n",
    "                sleep(2)\n",
    "                driver.find_element(By.CLASS_NAME, \"geetest_replay\").click()\n",
    "                audio_data = record_audio(duration=15)\n",
    "                sleep(2)\n",
    "                save_audio(audio_data)\n",
    "\n",
    "                client = OpenAI(api_key= api_key)\n",
    "                audio_file = os.getcwd() + \"\\\\recorded_audio.mp3\"\n",
    "\n",
    "                audio_file= open(audio_file, \"rb\")\n",
    "                transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
    "\n",
    "                codigo = transcript.text.replace(\"Introduzca lo que oiga.\", \"\").replace(\", \", \"\").replace(\".\", \"\").strip()\n",
    "\n",
    "                barra = driver.find_element(By.CLASS_NAME, \"geetest_input\")\n",
    "                barra.send_keys(codigo)\n",
    "                sleep(2)\n",
    "                driver.find_element(By.CLASS_NAME, \"geetest_box\").find_element(By.CLASS_NAME, \"geetest_btn\").click()\n",
    "                sleep(5)\n",
    "                ###########################\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        titulo = extraccion_datos_try_except_simple(soup, [('div', {'class' : 'heading-addons'}), ('h1', {\"id\": \"prefijoPuesto\"})])\n",
    "\n",
    "        empresa = extraccion_datos_try_except_simple(soup, [('div', {'class' : 'heading-addons'}), ('a', {\"class\": \"link\"})])\n",
    "\n",
    "        ubicacion = extraccion_datos_try_except_simple(soup, [('div', {'class' : 'row-matrioska'}), ('span', {\"id\": \"prefijoPoblacion\"})]).replace(\",\", \"\")\n",
    "\n",
    "        descripcion = extraccion_datos_try_except_simple(soup, [('div', {\"id\": \"prefijoDescripcion1\"})])\n",
    "\n",
    "        herramientas = extraccion_datos_try_except_compleja(soup, [(\"div\", {'class' : \"inner-expanded\"}), (\"ul\", {'class' : \"list-default\"}), ('a', {})], elementos_a_encontrar = 'lista_completa')\n",
    "\n",
    "        funciones = extraccion_datos_try_except_simple(soup, [(\"div\", {'class' : \"border-top\"}), (\"ul\", {}), ('span', {'class' : \"list-default-text\"})])\n",
    "        \n",
    "        try:\n",
    "            fecha = soup.find(\"div\", class_= \"row-matrioska\").find(\"span\", class_= \"marked\").text\n",
    "        except:\n",
    "            try:\n",
    "                lista_bullets = soup.find(\"div\", class_= \"row-matrioska\").find(\"ul\", class_= \"list-bullet-default\").find_all(\"li\")\n",
    "                for bullet in lista_bullets:\n",
    "                    tag_bullet = bullet.text.strip()[:9]\n",
    "                    if tag_bullet == \"Publicada\":\n",
    "                        fecha = bullet.text.strip()[12:]\n",
    "            except:\n",
    "                fecha = np.nan\n",
    "\n",
    "        attrs_ubic= {\"id\": \"prefijoPoblacion\"}\n",
    "       \n",
    "        try:\n",
    "            lista_li = soup.find(\"div\", class_= \"row-matrioska\").find_all(\"li\")        \n",
    "\n",
    "            for li in lista_li:\n",
    "\n",
    "                tag = li.find(\"span\").text[:7]\n",
    "\n",
    "                if tag == \"Salario\":\n",
    "                    salario = li.find(\"span\").text[8:]\n",
    "\n",
    "                elif tag == \"Experie\":\n",
    "                    experiencia = li.find(\"span\").text[20:]\n",
    "\n",
    "                elif tag == \"Tipo de\":\n",
    "\n",
    "                    tipo_contrato = li.find(\"span\").text[18:].split(\",\")[0]\n",
    "\n",
    "                    jornada = li.find(\"span\").text[18:].split(\",\")[-1]\n",
    "        except:\n",
    "            salario = np.nan\n",
    "            experiencia = np.nan\n",
    "            tipo_contrato = np.nan\n",
    "            jornada = np.nan\n",
    "\n",
    "        data.append([titulo, empresa, fecha, herramientas, descripcion, ubicacion, funciones, jornada, experiencia, tipo_contrato, salario, fecha_scrapeo, empleo, portal])\n",
    "        contador += 1\n",
    "\n",
    "        if contador % 100 == 0:\n",
    "            sleep(20)\n",
    "            \n",
    "    df_infojobs = pd.DataFrame(data)\n",
    "    df_infojobs = df_infojobs.dropna(how= \"all\")\n",
    "    data = df_infojobs.values\n",
    "    guardar_datos(portal = portal, data = data, nombres_columnas = nombres_columnas, ruta_datos = ruta_datos)\n",
    "    print(f'{portal} - Scrapeo completado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e72fd49-20e7-4a8b-9576-05ca2a09b1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prueba = pd.read_csv('C:\\\\Users\\\\regue\\\\Desktop\\\\Data Science Projects\\\\PROJECTS\\\\IT_Job_Spain_Project\\\\Datos\\\\datos_sin_procesar\\\\' + \"datos_infojobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13b23bdb-2ad6-42cc-8453-2d25213fefb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Full Stack Developer',\n",
       "        'Tokio Marine Europe S.A Sucursal en España  Products',\n",
       "        'hace 1h', ..., '2024-01-26', 'infojobs', nan],\n",
       "       ['Operador Experto en Sistema Cloud AWS', 'EZENTIS', 'hace 1h',\n",
       "        ..., '2024-01-26', 'infojobs', nan],\n",
       "       ['ARQUITECTO/A SOFTWARE TECNOLOGÍA FRONT', 'CASER - Corporativas',\n",
       "        'hace 1h', ..., '2024-01-26', 'infojobs', nan],\n",
       "       ...,\n",
       "       ['Técnicos/as de Soporte IT - Titulados/as en Grado Superior Informática/Telecomunicaciones',\n",
       "        'Grupo Zelenza', 'ace 1d', ..., '2024-02-10', 'infojobs',\n",
       "        'https://www.infojobs.net/madrid/tecnicos-soporte-it-titulados-grado-superior-informatica-telecomunicaciones/of-ia259d0ed6a44fbbceb36f724237f16?applicationOrigin=search-new&page=21&sortBy=PUBLICATION_DATE'],\n",
       "       ['Un/a Programador/a ERP Axional Deister', 'Clubs de Fitness DiR',\n",
       "        'ace 1d', ..., '2024-02-10', 'infojobs',\n",
       "        'https://www.infojobs.net/barcelona/un-programador-erp-axional-deister/of-i65d6b6687342ccb42475df3140400c?applicationOrigin=search-new&page=4&sortBy=PUBLICATION_DATE'],\n",
       "       ['Área  Soporte y parametrización Control Horario.',\n",
       "        'MHP Servicios de Control S.L', 'ace 1d', ..., '2024-02-10',\n",
       "        'infojobs',\n",
       "        'https://www.infojobs.net/las-palmas-de-gran-canaria/area-soporte-parametrizacion-control-horario./of-ibb85b102ed487c9cfc128b4a098641?applicationOrigin=search-new&page=24&sortBy=PUBLICATION_DATE']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prueba.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a9bdf-d2ba-45c1-abde-a2a9beefc9bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 5. Ticjob Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977949e-cb42-4969-8bde-f5366e245bc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.1. Búsqueda ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddcd27d4-7522-45da-b139-714927982e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def busqueda_ticjob(driver, dic_datos = None):\n",
    "    # Ordenamos los resultados por fecha\n",
    "    driver.find_element(By.CLASS_NAME, 'sort-by-date-container').click()\n",
    "\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38681da2-03e0-463f-b0ce-5b3b61124408",
   "metadata": {},
   "source": [
    "## 5.2 Información ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba5edc6b-1646-4655-9f4e-d00f14bd92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_ticjob(driver, fecha_scrapeo, portal, nombres_columnas, ruta_datos, dic_datos):\n",
    "    intervalo_temporal_busqueda = dic_datos['intervalo_temporal_busqueda']\n",
    "    intervalo_temporal_busqueda = timedelta(days = intervalo_temporal_busqueda)\n",
    "    \n",
    "    # Ordenamos los resultados por fecha\n",
    "    driver.find_element(By.CLASS_NAME, 'sort-by-date-container').click()\n",
    "\n",
    "    sleep(5)\n",
    "    \n",
    "    data = []\n",
    "    final_scrapeo = False\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        ofertas = driver.find_elements(By.CLASS_NAME, 'job-card')\n",
    "\n",
    "        for oferta in ofertas:\n",
    "            #Extraemos los datos con selenium, ya que al obtener la url de la pagina nos da los de una busqueda generica sin nuetros criterios\n",
    "            #Extraemos solo los datos del intervalo temporal seleccionado\n",
    "            fecha = oferta.find_element(By.CSS_SELECTOR, 'div[class = \"job-card-label date-field\"]').text\n",
    "            fecha = datetime.strptime(fecha, '%d/%m/%Y').date()\n",
    "\n",
    "            if fecha < fecha_scrapeo - intervalo_temporal_busqueda:\n",
    "                final_scrapeo = True\n",
    "                break\n",
    "\n",
    "            url = oferta.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "\n",
    "            response = requests.get(url)\n",
    "\n",
    "            #Extraemos los datos con soup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            titulo = extraccion_datos_try_except(soup, 'h1', {'id' : 'job-title'})\n",
    "\n",
    "            empresa = soup.find('a', class_ = 'company-image')['title']\n",
    "\n",
    "            descripcion = extraccion_datos_try_except(soup, 'div', {'class' : 'job-offer job-offer-content'})\n",
    "\n",
    "            ubicacion = extraccion_datos_try_except(soup, 'li', {'class' : 'multi-job-location-apply'})\n",
    "            if (ubicacion != np.nan) and (';' in ubicacion):\n",
    "                ubicacion = ubicacion.split(';')\n",
    "\n",
    "            experiencia = extraccion_datos_try_except(soup, 'li', {'id' : 'summaryExp'})\n",
    "\n",
    "            localizacion = soup.find('li', class_ = 'multi-job-location-apply')\n",
    "            tipo_contrato = localizacion.find_next('li').get_text(strip = True)\n",
    "\n",
    "            salario = extraccion_datos_try_except(soup, 'li', {'id' : 'summarySalary'})\n",
    "            # Eliminamos los salarios == '0' ya que no son datos reales que se muestren en la pagina, ya que corresponden con campos vacios\n",
    "            salario = salario if salario != '0' else np.nan\n",
    "\n",
    "            herramientas = soup.find('div', class_ = 'search-criteria-tags').find_all('a')\n",
    "            herramientas = [herramienta.text for herramienta in herramientas]\n",
    "\n",
    "            data.append([titulo, empresa, fecha, herramientas, descripcion, ubicacion, experiencia, tipo_contrato, salario, fecha_scrapeo, url, portal])\n",
    "            sleep(1)\n",
    "\n",
    "\n",
    "        #Guardamos cada vez que termina de sacrapear una pagina\n",
    "        guardar_datos(portal = portal, nombres_columnas = nombres_columnas, data = data, ruta_datos = ruta_datos)\n",
    "        data = []\n",
    "\n",
    "        paginas_totales = driver.find_element(By.CLASS_NAME, 'page-list').text.split('\\n')[-1]\n",
    "        paginas_totales = int(paginas_totales)\n",
    "        pagina_actual = int(driver.find_element(By.CLASS_NAME, 'current').text)\n",
    "\n",
    "        # Pasamos a la siguiente pagina hasta llegar a la ultima\n",
    "        if (paginas_totales != pagina_actual) and not final_scrapeo:\n",
    "            # Pasamos a la siguiente pagina\n",
    "            siguiente_pagina = driver.find_element(By.CLASS_NAME, 'next')\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", siguiente_pagina)\n",
    "            siguiente_pagina.click()\n",
    "            sleep(2)\n",
    "        else:\n",
    "            print(f'{portal} - Scrapeo completado')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d0f007-222c-489b-8bc1-0cf42eea8180",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 6. Indeed Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b67cb4-e3e6-464d-8088-d4f55d46874a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6.1. Búsqueda ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "070f4a6a-405d-431d-8da8-2c68269385e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def busqueda_indeed(driver, empleo, dic_datos):\n",
    "    ubicacion_a_buscar = dic_datos['ubicacion_a_buscar']\n",
    "    \n",
    "    intervalo_temporal_busqueda = dic_datos['intervalo_temporal_busqueda']\n",
    "    \n",
    "    # Buscar empleo\n",
    "    buscador_empleo = driver.find_element(By.ID, 'text-input-what')\n",
    "    buscador_empleo.clear()\n",
    "    sleep(1)\n",
    "    buscador_empleo.send_keys(empleo)\n",
    "    sleep(1)\n",
    "\n",
    "    # Buscar ubicacion\n",
    "    buscador_ubicacion = driver.find_element(By.ID, 'text-input-where')\n",
    "    #buscador_ubicacion.send_keys(ubicacion_a_buscar)\n",
    "    sleep(1)\n",
    "    buscador_ubicacion.send_keys(Keys.ENTER)\n",
    "    sleep(1)\n",
    "\n",
    "    #Seleccionamos la fecha de publicacion\n",
    "    driver.find_element(By.ID, 'filter-dateposted').click()\n",
    "    sleep(2)\n",
    "    fechas_publicacion = driver.find_element(By.ID, 'filter-dateposted-menu')\n",
    "    opciones_fechas_publicacion = fechas_publicacion.text.split('\\n')\n",
    "\n",
    "    #Seleccionamos los elementos interactuables de las fechas de publicacion\n",
    "    botones_fechas_publicacion = fechas_publicacion.find_elements(By.TAG_NAME, 'li')\n",
    "\n",
    "    #Pulsamos el que tiene el intervalo de tiempo seleccionado\n",
    "    [publicaciones_seleccionadas for publicaciones_seleccionadas in  botones_fechas_publicacion if intervalo_temporal_busqueda in publicaciones_seleccionadas.text][0].click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae1ecf-a023-41b0-ad2a-228818bcb190",
   "metadata": {},
   "source": [
    "## 6.2 Información ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e34e4bc5-b9d8-494c-8a52-1162ba2cca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_indeed(empleo, driver, fecha_scrapeo, portal, nombres_columnas, ruta_datos, dic_datos):\n",
    "    tipos_trabajos = dic_datos['tipos_trabajos']\n",
    "    \n",
    "    while True:\n",
    "        data = []\n",
    "        sleep(4)\n",
    "        try:\n",
    "            #Cerrar banner\n",
    "            banner = driver.find_element(By.CSS_SELECTOR, 'button[aria-label = \"cerrar\"]')\n",
    "            banner.click()\n",
    "\n",
    "        except:\n",
    "            next\n",
    "\n",
    "        ofertas = driver.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
    "        for oferta in ofertas:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", oferta)\n",
    "            sleep(2)\n",
    "            oferta.click()\n",
    "            sleep(3)\n",
    "\n",
    "            titulo = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'h2[data-testid = \"jobsearch-JobInfoHeader-title\"]')))\n",
    "            titulo = titulo.text.split('\\n')[0]\n",
    "\n",
    "            empresa = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'div[data-testid = \"inlineHeader-companyName\"]')))\n",
    "            empresa = empresa.text\n",
    "\n",
    "            descripcion = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "            descripcion = descripcion.text.replace('\\n', ' ')\n",
    "\n",
    "            fecha = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'span[data-testid = \"myJobsStateDate\"]')))\n",
    "            fecha = fecha.text.split('\\n')[1]\n",
    "\n",
    "            ubicacion_presencialidad = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'div[data-testid = \"inlineHeader-companyLocation\"]')))\n",
    "            ubicacion_presencialidad = ubicacion_presencialidad.text.split('•')\n",
    "\n",
    "            if len(ubicacion_presencialidad) > 1:\n",
    "                ubicacion, presencialidad = ubicacion_presencialidad\n",
    "            else:\n",
    "                if ubicacion_presencialidad in tipos_trabajos:\n",
    "                    presencialidad = ubicacion_presencialidad[0]\n",
    "                    ubicacion = np.nan\n",
    "                else:\n",
    "                    ubicacion = ubicacion_presencialidad[0]\n",
    "                    presencialidad = np.nan\n",
    "            try:\n",
    "                beneficios = driver.find_element(By.ID, 'benefits').text\n",
    "                beneficios = beneficios.replace('Beneficios\\nObtenidos de la descripción completa del empleo\\n','').split('\\n')\n",
    "\n",
    "            except:\n",
    "                beneficios = np.nan\n",
    "                next\n",
    "\n",
    "            detalles_a_buscar = {'Salario' : np.nan, 'Tipo de empleo' : np.nan}   \n",
    "\n",
    "            try:\n",
    "                detalles = driver.find_element(By.ID, 'jobDetailsSection')\n",
    "                detalles = detalles.text.split('\\n')    \n",
    "\n",
    "                for detalle in detalles_a_buscar.keys():\n",
    "                    if detalle in detalles:\n",
    "                        idx = detalles.index(detalle)\n",
    "                        detalles_a_buscar[detalle] = detalles[idx + 1]\n",
    "            except:\n",
    "                next\n",
    "\n",
    "            salario, jornada = detalles_a_buscar.values()\n",
    "\n",
    "            url = driver.current_url\n",
    "\n",
    "            data.append([titulo, empresa, fecha, descripcion, ubicacion, jornada, presencialidad, salario, beneficios, fecha_scrapeo, url, portal])\n",
    "            \n",
    "        #Guardamos cada vez que termina de sacrapear una pagina\n",
    "        guardar_datos(portal = portal, nombres_columnas = nombres_columnas, data = data, ruta_datos = ruta_datos)\n",
    "\n",
    "        pagina_actual = int(driver.find_element(By.CSS_SELECTOR, 'a[data-testid = \"pagination-page-current\"]').text)\n",
    "\n",
    "        try:\n",
    "            driver.find_element(By.CSS_SELECTOR, f'a[data-testid = \"pagination-page-{pagina_actual + 1}\"]').click()\n",
    "            sleep(3)\n",
    "        except:\n",
    "            print(f'{portal} - Scrapeo completado para {empleo}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3834f-3feb-446a-8e8d-83214b8bdff7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 7. Linkedin Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b871cb-548d-4992-9fea-fa5ebdedfee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7.1. Búsqueda ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80f0a81d-398f-42fa-a8f1-80564bffe26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def busqueda_linkedin(driver, empleo, dic_datos):    \n",
    "    usuario = dic_datos['usuario']\n",
    "    password = dic_datos['password']\n",
    "    intervalo_temporal_busqueda = dic_datos['intervalo_temporal_busqueda']\n",
    "    try:\n",
    "        #Iniciamos sesion\n",
    "        driver.find_element(By.ID, value = 'session_key').send_keys(usuario)\n",
    "        sleep(1)\n",
    "        driver.find_element(By.ID, value = 'session_password').send_keys(password)\n",
    "        sleep(1)\n",
    "        driver.find_element(By.CSS_SELECTOR, value = 'button[data-id = \"sign-in-form__submit-btn\"').click()\n",
    "        sleep(2)\n",
    "    \n",
    "    except:\n",
    "        next\n",
    "        \n",
    "    #Accedemos a los empleos\n",
    "    empleos = WebDriverWait(driver, 120).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'span[title = \"Empleos\"]')))\n",
    "    empleos.click()\n",
    "    sleep(2)\n",
    "\n",
    "    #Buscamos empleo\n",
    "    buscador = driver.find_element(By.CSS_SELECTOR, value = 'input[class = \"jobs-search-box__text-input jobs-search-box__keyboard-text-input\"]')\n",
    "    buscador.send_keys(empleo)\n",
    "    sleep(1)\n",
    "    buscador.send_keys(Keys.ENTER)\n",
    "    sleep(2)\n",
    "\n",
    "    #Pulsamos todos los filtros\n",
    "    todos_los_filtros = WebDriverWait(driver, 120).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'div[class = \"relative mr2\"]')))\n",
    "    todos_los_filtros.click()\n",
    "\n",
    "    # Localizamos las fechas de publicacion de los empleos\n",
    "    fechas_publicacion = [filtro for filtro in driver.find_elements(By.CLASS_NAME, value = 'search-reusables__secondary-filters-filter') if 'Fecha de publicación' in filtro.text][0]\n",
    "    opciones_fechas_publicacion = fechas_publicacion.text.split('\\n')\n",
    "    opciones_fechas_publicacion = [opcion_fechas_publicacion for opcion_fechas_publicacion in opciones_fechas_publicacion if '«' not in opcion_fechas_publicacion][1:]\n",
    "\n",
    "    # Obtener el valor seleccionado por el usuario\n",
    "    '''publicaciones_seleccionadas = [publicaciones_seleccionadas for publicaciones_seleccionadas in fechas_publicacion.find_elements(By.TAG_NAME, 'li') if intervalo_temporal_busqueda in publicaciones_seleccionadas.text][0]\n",
    "    publicaciones_seleccionadas.find_element(By.TAG_NAME, 'label').click()\n",
    "    '''\n",
    "    # Extraemos los tipos de empleos disponibles para la busqueda\n",
    "    tipos_empleos = [filtro.text for filtro in driver.find_elements(By.CLASS_NAME, value = 'search-reusables__secondary-filters-filter') if 'Tipo de empleo' in filtro.text][0].split('\\n')\n",
    "    tipos_empleos = [tipo for tipo in tipos_empleos if '«' not in tipo][1:]\n",
    "\n",
    "    # Extraemos los tipos_presencialidad disponibles para la busqueda\n",
    "    tipos_presencialidad = [filtro.text for filtro in driver.find_elements(By.CLASS_NAME, value = 'search-reusables__secondary-filters-filter') if 'En remoto' in filtro.text][0].split('\\n')\n",
    "    tipos_presencialidad = [presencialidad for presencialidad in tipos_presencialidad if '«' not in presencialidad][1:]\n",
    "\n",
    "    sleep(1)\n",
    "    \n",
    "    #Cerramos los filtros\n",
    "    cerrar_filtros = [elemento for elemento in driver.find_elements(By.CSS_SELECTOR, 'span[class = \"a11y-text\"]') if 'Todos los filtros' in elemento.text][0]\n",
    "    cerrar_filtros.find_element(By.XPATH, \".//following-sibling::button\").click()\n",
    "    sleep(1)\n",
    "     \n",
    "    # Seleccion fechas publicacion\n",
    "    fechas_publicacion = [filtro for filtro in driver.find_elements(By.CLASS_NAME, 'search-reusables__primary-filter') if 'Fecha' in filtro.text][0]\n",
    "    fechas_publicacion.click()\n",
    "\n",
    "    publicaciones_seleccionadas = [filtro for filtro in fechas_publicacion.find_elements(By.CLASS_NAME, 'search-reusables__value-label') if intervalo_temporal_busqueda in filtro.text][0]\n",
    "    publicaciones_seleccionadas.click()\n",
    "\n",
    "    botones = fechas_publicacion.find_element(By.XPATH, \".//following-sibling::div\")\n",
    "\n",
    "    botones.find_elements(By.TAG_NAME, 'button')[-1].click()\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ef6f1-3fbd-4c1b-8752-b3e5ecf418d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 7.2 Información ofertas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd103031-4710-4e85-9306-f736e8a1e541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraper_linkedin(empleo, driver, fecha_scrapeo, portal, nombres_columnas, ruta_datos, dic_datos):\n",
    "\n",
    "    while True:\n",
    "        data = []\n",
    "        sleep(4)\n",
    "\n",
    "        # Obtenemos todas las ofertas de la pagina actual\n",
    "        while True:\n",
    "            ofertas_visibles = driver.find_elements(By.CSS_SELECTOR, value = 'div[data-view-name = \"job-card\"]')\n",
    "            if not ofertas_visibles:\n",
    "                print(f'{portal} - Scrapeo completado para {empleo}')\n",
    "                break\n",
    "\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", ofertas_visibles[-1])\n",
    "            sleep(1)\n",
    "            nuevas_ofertas_visibles = driver.find_elements(By.CSS_SELECTOR, value = 'div[data-view-name = \"job-card\"]')\n",
    "            if len(nuevas_ofertas_visibles) != len(ofertas_visibles):\n",
    "                next\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        #Extraccion de datos de cada oferta\n",
    "        for oferta in ofertas_visibles:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", oferta)\n",
    "            sleep(2)\n",
    "            oferta.click()\n",
    "            sleep(2)\n",
    "            datos = driver.find_element(By.CSS_SELECTOR, value = 'div[class = \"job-details-jobs-unified-top-card__primary-description-without-tagline mb2\"]').text\n",
    "            sleep(2)\n",
    "\n",
    "            # try except para no perder datos, ya que si no se consigue realizar la accion se volvera a la oferta mas tarde\n",
    "            try:\n",
    "                titulo, empresa = oferta.text.split('\\n')[:2]\n",
    "            except:\n",
    "                ofertas_visibles.extend([oferta])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ubicacion, fecha, solicitudes = datos.split('·')[1:]\n",
    "            except:\n",
    "                ubicacion, fecha, solicitudes = np.nan, np.nan, np.nan\n",
    "\n",
    "            descripcion = driver.find_element(By.ID, value = 'job-details').text.replace('\\n', '')\n",
    "\n",
    "            jornada_presencialidad = driver.find_elements(By.CLASS_NAME, value = 'job-details-jobs-unified-top-card__job-insight')[0].text\n",
    "            try:\n",
    "                jornada = [jornada for jornada in tipos_empleos if jornada in jornada_presencialidad][0]\n",
    "            except:\n",
    "                jornada = np.nan\n",
    "                next\n",
    "            try:\n",
    "                presencialidad = [presencialidad for presencialidad in tipos_presencialidad if presencialidad in jornada_presencialidad][0]\n",
    "            except:\n",
    "                presencialidad = np.nan\n",
    "                next\n",
    "            #mostrar todas las aptitudes\n",
    "            while True:\n",
    "                try:\n",
    "                    #Tratamos de buscar si hay aptitudes\n",
    "                    try:\n",
    "                        [enlace_aptitudes for enlace_aptitudes in driver.find_elements(By.CLASS_NAME, value = 'app-aware-link') if 'Aptitudes' in enlace_aptitudes.text][0].click()\n",
    "                        sleep(2)\n",
    "\n",
    "                        [mostrar_aptitudes for mostrar_aptitudes in driver.find_elements(By.CLASS_NAME, value = 'artdeco-button__text') if mostrar_aptitudes.text == 'Mostrar todas las aptitudes'][0].click()\n",
    "                        sleep(2)\n",
    "\n",
    "                        try:\n",
    "                            caja_aptitudes = driver.find_element(By.CLASS_NAME, value = 'job-details-skill-match-status-list')\n",
    "\n",
    "                        except NoSuchElementException:\n",
    "                            #Salir de la caja de aptitudes\n",
    "                            [boton for boton in driver.find_elements(By.CLASS_NAME, value = 'artdeco-button__text') if boton.text == 'Done'][0].click()\n",
    "                            continue\n",
    "\n",
    "                        sleep(4)\n",
    "                        herramientas = [aptitud.text.replace('\\nAñadir', '') for aptitud in caja_aptitudes.find_elements(By.TAG_NAME, value = 'li')]\n",
    "\n",
    "                        sleep(2)\n",
    "\n",
    "                        #Salir de la caja de aptitudes\n",
    "                        [boton for boton in driver.find_elements(By.CLASS_NAME, value = 'artdeco-button__text') if boton.text == 'Done'][0].click()\n",
    "\n",
    "                        break\n",
    "                    except StaleElementReferenceException:\n",
    "                        next\n",
    "                except:\n",
    "\n",
    "                    heramientas = np.nan\n",
    "                    break\n",
    "\n",
    "            url = driver.current_url\n",
    "\n",
    "            data.append([titulo, empresa, fecha, herramientas, descripcion, ubicacion, jornada, presencialidad, solicitudes, fecha_scrapeo, url, portal])\n",
    "\n",
    "        #Guardamos cada vez que termina de sacrapear una pagina\n",
    "        guardar_datos(portal = portal, nombres_columnas = nombres_columnas, data = data, ruta_datos = ruta_datos)\n",
    "\n",
    "        pagina_actual = driver.find_element(By.CSS_SELECTOR, 'li[class = \"artdeco-pagination__indicator artdeco-pagination__indicator--number active selected ember-view\"]')\n",
    "\n",
    "        try:\n",
    "            pagina_actual.find_element(By.XPATH, \".//following-sibling::li\").click()\n",
    "\n",
    "        except:\n",
    "            print(f'{portal} - Scrapeo completado para {empleo}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d4df5-7b7a-4d1c-ba2c-af67a45e4c76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Datos JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b2bf09d-bfad-46e4-a4d2-b5ab741f0af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diccionario_datos = {\n",
    "    'portales_busqueda': {\n",
    "        'ticjob': {\n",
    "            'busqueda_por_empleo' : False,\n",
    "            'busqueda_por_url' : False,\n",
    "           \n",
    "            'url': 'https://ticjob.es/esp/busqueda',\n",
    "            'nombres_columnas': [\"titulo\", \"empresa\", \"fecha\", \"herramientas\", \"descripcion\", \"ubicacion\", \"experiencia\", \"tipo_contrato\", \"salario\", \"fecha_scrapeo\", \"url\", \"portal\"],\n",
    "            \n",
    "            'dic_datos' : {'intervalo_temporal_busqueda': 10,\n",
    "                           'opciones_fechas_publicacion' : None}\n",
    "        },\n",
    " \n",
    "        'indeed': {\n",
    "            'busqueda_por_empleo' : True,\n",
    "            'busqueda_por_url' : False,\n",
    "            \n",
    "            'url': 'https://es.indeed.com/',\n",
    "            'nombres_columnas': ['titulo', 'empresa', 'fecha', 'descripcion', 'ubicacion', 'jornada', 'presencialidad', 'salario', 'beneficios', 'fecha_scrapeo', 'url', 'portal'],\n",
    "            \n",
    "            'dic_datos' : {'ubicacion_a_buscar': 'España',\n",
    "                           'intervalo_temporal_busqueda': 'Últimos 7 días',\n",
    "                           'opciones_fechas_publicacion': ['Últimas 24 horas', 'Últimos 3 días', 'Últimos 7 días', 'Últimos 14 días'],\n",
    "                           'tipos_trabajos' : ('Remoto híbrido', 'Teletrabajo')}\n",
    "        },\n",
    "\n",
    "        'linkedin': {\n",
    "            'busqueda_por_empleo' : True,\n",
    "            'busqueda_por_url' : False,\n",
    "            \n",
    "            'url': 'https://www.linkedin.com/',\n",
    "            'nombres_columnas': ['titulo', 'empresa', 'fecha', 'herramientas', 'descripcion', 'ubicacion', 'jornada', 'presencialidad', 'solicitudes', 'fecha_scrapeo', 'url', 'portal'],\n",
    "            \n",
    "            'dic_datos' : {'usuario' : None,\n",
    "                           'password' : None,\n",
    "                           'intervalo_temporal_busqueda': 'Semana pasada',\n",
    "                           'opciones_fechas_publicacion': ['Últimas 24 horas', 'Semana pasada']}\n",
    "            \n",
    "        },\n",
    "  \n",
    "        'infoempleo': {\n",
    "            'busqueda_por_empleo' : False,\n",
    "            'busqueda_por_url' : True,\n",
    "            \n",
    "            'url': 'https://www.infoempleo.com/trabajo/area-de-empresa_tecnologia-e-informatica/',\n",
    "            'nombres_columnas': [\"titulo\", \"empresa\", \"fecha\", \"herramientas\", \"descripcion\", \"ubicacion\", \"presencialidad\", \"funciones\", \"jornada\", \"experiencia\", \"tipo_contrato\", \"salario\", \"solicitudes\", \"fecha_scrapeo\", \"url\", \"portal\"],\n",
    "            \n",
    "            'dic_datos' : {'opcion' : 'fechapublicacion3',\n",
    "                           'opciones_fechas_publicacion' : ['fechapublicacion1', 'fechapublicacion2', 'fechapublicacion3']}\n",
    "        },\n",
    "     \n",
    "        'talenthacker': {\n",
    "            'busqueda_por_empleo' : False,\n",
    "            'busqueda_por_url' : True,\n",
    "            \n",
    "            'url': 'https://talenthackers.net/spots/',\n",
    "            'nombres_columnas': [\"titulo\", \"herramientas\", \"descripcion\", \"ubicacion\", \"presencialidad\", \"funciones\", \"jornada\", \"experiencia\", \"tipo_contrato\", \"salario\", \"fecha_scrapeo\", \"url\", \"portal\"],\n",
    "            \n",
    "            'dic_datos' : {'bullet_point' : {'ubicacion': 'data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJDOC4xMyAyIDUgNS4xMyA1IDlDNSAxNC4yNSAxMiAyMiAxMiAyMkMxMiAyMiAxOSAxNC4yNSAxOSA5QzE5IDUuMTMgMTUuODcgMiAxMiAyWk03IDlDNyA2LjI0IDkuMjQgNCAxMiA0QzE0Ljc2IDQgMTcgNi4yNCAxNyA5QzE3IDExLjg4IDE0LjEyIDE2LjE5IDEyIDE4Ljg4QzkuOTIgMTYuMjEgNyAxMS44NSA3IDlaIiBmaWxsPSIjNjI3QTc4Ii8+CjxwYXRoIGQ9Ik0xMiAxMS41QzEzLjM4MDcgMTEuNSAxNC41IDEwLjM4MDcgMTQuNSA5QzE0LjUgNy42MTkyOSAxMy4zODA3IDYuNSAxMiA2LjVDMTAuNjE5MyA2LjUgOS41IDcuNjE5MjkgOS41IDlDOS41IDEwLjM4MDcgMTAuNjE5MyAxMS41IDEyIDExLjVaIiBmaWxsPSIjNjI3QTc4Ii8+Cjwvc3ZnPgo=',\n",
    "                                             'presencialidad': 'data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTE3IDE1SDE5VjE3SDE3VjE1Wk0xNyAxMUgxOVYxM0gxN1YxMVpNMTcgN0gxOVY5SDE3VjdaTTEzLjc0IDdMMTUgNy44NFY3SDEzLjc0WiIgZmlsbD0iIzYyN0E3OCIvPgo8cGF0aCBkPSJNMTAgM1Y0LjUxTDEyIDUuODRWNUgyMVYxOUgxN1YyMUgyM1YzSDEwWiIgZmlsbD0iIzYyN0E3OCIvPgo8cGF0aCBkPSJNOC4xNyA1LjdMMTUgMTAuMjVWMjFIMVYxMC40OEw4LjE3IDUuN1pNMTAgMTlIMTNWMTEuMTZMOC4xNyA4LjA5TDMgMTEuMzhWMTlINlYxM0gxMFYxOVoiIGZpbGw9IiM2MjdBNzgiLz4KPC9zdmc+Cg==',\n",
    "                                             'experiencia': 'data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjAiIGhlaWdodD0iMjAiIHZpZXdCb3g9IjAgMCAyMCAyMCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDVIMTIuNVY0LjVWMi41VjJIMTJIOEg3LjVWMi41VjQuNVY1SDhIMTJaTTIgNkgxLjVWNi41VjE3LjVWMThIMkgxOEgxOC41VjE3LjVWNi41VjZIMThIMlpNMC41MSA2LjUwMDQ1VjYuNUMwLjUxIDUuNjYyODYgMS4xNjk0MSA1IDIgNUg2SDYuNVY0LjVWMi41QzYuNSAxLjY2NjE0IDcuMTY2MTQgMSA4IDFIMTJDMTIuODMzOSAxIDEzLjUgMS42NjYxNCAxMy41IDIuNVY0LjVWNUgxNEgxOEMxOC44MzM5IDUgMTkuNSA1LjY2NjE0IDE5LjUgNi41VjE3LjVDMTkuNSAxOC4zMzM5IDE4LjgzMzkgMTkgMTggMTlIMkMxLjE2NjIxIDE5IDAuNTAwMTExIDE4LjMzNCAwLjUgMTcuNTAwMkMwLjUgMTcuNTAwMSAwLjUgMTcuNTAwMSAwLjUgMTcuNUwwLjUxIDYuNTAwNDVaIiBmaWxsPSIjNjI3QTc4IiBzdHJva2U9IiM2MjdBNzgiLz4KPC9zdmc+Cg==',\n",
    "                                             'jornada_tipo': 'data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTcgMTZIMTRWMThIN1YxNlpNNyAxMkgxN1YxNEg3VjEyWk03IDhIMTdWMTBIN1Y4Wk0xOSA0SDE0LjgyQzE0LjQgMi44NCAxMy4zIDIgMTIgMkMxMC43IDIgOS42IDIuODQgOS4xOCA0SDVDNC44NiA0IDQuNzMgNC4wMSA0LjYgNC4wNEM0LjIxIDQuMTIgMy44NiA0LjMyIDMuNTkgNC41OUMzLjQxIDQuNzcgMy4yNiA0Ljk5IDMuMTYgNS4yM0MzLjA2IDUuNDYgMyA1LjcyIDMgNlYyMEMzIDIwLjI3IDMuMDYgMjAuNTQgMy4xNiAyMC43OEMzLjI2IDIxLjAyIDMuNDEgMjEuMjMgMy41OSAyMS40MkMzLjg2IDIxLjY5IDQuMjEgMjEuODkgNC42IDIxLjk3QzQuNzMgMjEuOTkgNC44NiAyMiA1IDIySDE5QzIwLjEgMjIgMjEgMjEuMSAyMSAyMFY2QzIxIDQuOSAyMC4xIDQgMTkgNFpNMTIgMy43NUMxMi40MSAzLjc1IDEyLjc1IDQuMDkgMTIuNzUgNC41QzEyLjc1IDQuOTEgMTIuNDEgNS4yNSAxMiA1LjI1QzExLjU5IDUuMjUgMTEuMjUgNC45MSAxMS4yNSA0LjVDMTEuMjUgNC4wOSAxMS41OSAzLjc1IDEyIDMuNzVaTTE5IDIwSDVWNkgxOVYyMFoiIGZpbGw9IiM2MjdBNzgiLz4KPC9zdmc+Cg==',\n",
    "                                             'idioma': 'data:image/svg+xml;base64,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',\n",
    "                                             'salario': 'data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjAiIGhlaWdodD0iMjAiIHZpZXdCb3g9IjAgMCAyMCAyMCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEwLjczIDQuNzFWNS4xMDE1NkwxMS4xMTAxIDUuMTk1NDJDMTIuMjg0NCA1LjQ4NTM3IDEyLjk0OTkgNi4yNjQzMSAxMy4xNTQxIDcuMThIMTIuNDUyQzEyLjM2OTcgNi43ODU5NyAxMi4xOTU2IDYuNDE4NjYgMTEuODg0IDYuMTI3MjdDMTEuNDUxIDUuNzIyNDQgMTAuODMzMiA1LjU0IDEwLjA3IDUuNTRDOS4zNDcwMiA1LjU0IDguNzExOTUgNS43MDE5MiA4LjI0MjMgNi4wMzAxOEM3Ljc1ODkyIDYuMzY4MDUgNy40NyA2Ljg3Mjk1IDcuNDcgNy40N0M3LjQ3IDguMDAzNzMgNy42OTYwNyA4LjQ1MzU5IDguMTc4MjQgOC44MTE0OUM4LjYyMzI5IDkuMTQxODMgOS4yODY5MSA5LjM5NTY5IDEwLjE4NTEgOS42MjQxNUMxMS4wNTUgOS44NTE1OSAxMS44ODUxIDEwLjEzNzIgMTIuNDk1NCAxMC41OTY5QzEzLjA3ODIgMTEuMDM1OCAxMy40NjUgMTEuNjM0NCAxMy40NyAxMi41NTk2QzEzLjQ2NTIgMTMuMjI1MSAxMy4yMjA4IDEzLjcxMjYgMTIuODMxNSAxNC4wNzMyQzEyLjQyOTQgMTQuNDQ1NiAxMS44NDYgMTQuNzA1NCAxMS4xNDYzIDE0LjgzODlMMTAuNzQgMTQuOTE2NFYxNS4zM1YxNi41SDkuNFYxNS4zVjE0Ljg5MjhMOS4wMDExOSAxNC44MTAzQzcuODIwNDkgMTQuNTY2MyA2Ljg5ODk0IDEzLjkxMDkgNi42MjA1NiAxMi44M0g3LjM0NzA0QzcuNDYwNjIgMTMuMjI3MyA3LjY3OTg2IDEzLjU5NjIgOC4wNDE0IDEzLjg4NTRDOC41MjgxNyAxNC4yNzQ4IDkuMjEwODUgMTQuNDcgMTAuMDggMTQuNDdDMTEuMDE1MyAxNC40NyAxMS42NzU0IDE0LjIzNDIgMTIuMTA2NSAxMy44NDQ3QzEyLjUzODcgMTMuNDU0MiAxMi42OCAxMi45NjMyIDEyLjY4IDEyLjU4QzEyLjY4IDEyLjExMTcgMTIuNTQ1MiAxMS42MTYgMTIuMDgzMyAxMS4xODI4QzExLjY0ODcgMTAuNzc1MyAxMC45NjQzIDEwLjQ2MTYgOS45NTU4NSAxMC4yMjM2QzguODk3ODUgOS45NzAwMiA4LjA2Njk4IDkuNjM2NDQgNy41MDgxMyA5LjE5MDQ1QzYuOTcyMzggOC43NjI4OCA2LjY4IDguMjI5NSA2LjY4IDcuNUM2LjY4IDYuMzIzMTQgNy42MTQzNCA1LjQ3MzU2IDkuMDAzNjYgNS4xNzkxNEw5LjQgNS4wOTUxNFY0LjY5VjMuNUgxMC43M1Y0LjcxWk0wLjUgMTBDMC41IDQuNzU2MTQgNC43NTYxNCAwLjUgMTAgMC41QzE1LjI0MzkgMC41IDE5LjUgNC43NTYxNCAxOS41IDEwQzE5LjUgMTUuMjQzOSAxNS4yNDM5IDE5LjUgMTAgMTkuNUM0Ljc1NjE0IDE5LjUgMC41IDE1LjI0MzkgMC41IDEwWk0xLjUgMTBDMS41IDE0LjY4NjEgNS4zMTM4NiAxOC41IDEwIDE4LjVDMTQuNjg2MSAxOC41IDE4LjUgMTQuNjg2MSAxOC41IDEwQzE4LjUgNS4zMTM4NiAxNC42ODYxIDEuNSAxMCAxLjVDNS4zMTM4NiAxLjUgMS41IDUuMzEzODYgMS41IDEwWiIgZmlsbD0iIzYyN0E3OCIgc3Ryb2tlPSIjNjI3QTc4Ii8+Cjwvc3ZnPgo='}}\n",
    "        },\n",
    "     \n",
    "        'tecnoempleo': {\n",
    "            'busqueda_por_empleo' : False,\n",
    "            'busqueda_por_url' : True,\n",
    "            \n",
    "            'url': 'about:blank',\n",
    "            'nombres_columnas': [\"titulo\", \"empresa\", \"fecha\", \"herramientas\", \"descripcion\", \"ubicacion\", \"funciones\", \"jornada\", \"experiencia\", \"tipo_contrato\", \"salario\", \"fecha_scrapeo\", \"url\", \"portal\"],\n",
    "            \n",
    "            'dic_datos' : {'limite' : 1000,\n",
    "                           'nuevas' : True}\n",
    "        },\n",
    "      \n",
    "        'infojobs': {\n",
    "            'busqueda_por_empleo' : False,\n",
    "            'busqueda_por_url' : True,\n",
    "            \n",
    "            'url': 'about:blank',\n",
    "            'nombres_columnas': [\"titulo\", \"empresa\", \"fecha\", \"herramientas\", \"descripcion\", \"ubicacion\", \"funciones\", \"jornada\", \"experiencia\", \"tipo_contrato\", \"salario\", \"fecha_scrapeo\", \"url\", \"portal\"],\n",
    "            \n",
    "            'dic_datos' : {'api_key' : None,\n",
    "                           'opcion' : '_15_DAYS',\n",
    "                           'opciones_fechas_publicacion' : ['_24_HOURS', '_7_DAYS', '_15_DAYS']}\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    \n",
    "    'empleos_a_buscar': [\n",
    "            \"Desarrollador de software\", \"Ingeniero de desarrollo\", \"Analista de sistemas\", \"Desarrollador web\", \"Ingeniero DevOps\", \"Administrador de bases de datos\", \"Científico de datos\", \"Ingeniero de aprendizaje automático\", \"Desarrollador de aplicaciones\", \"Analista de seguridad informática\", \"Ingeniero de redes\",\n",
    "            \"Ingeniero de sistemas\", \"Desarrollador de juegos\", \"Analista de negocios de TI\", \"Arquitecto de software\", \"Ingeniero de pruebas de software\", \"Desarrollador de interfaces de usuario\", \"Ingeniero de automatización\", \"Especialista en análisis de rendimiento\", \"Ingeniero de realidad extendida\"\n",
    "            \"Software Developer\", \"Development Engineer\", \"Systems Analyst\", \"Web Developer\", \"DevOps Engineer\", \"Database Administrator\", \"Data Scientist\", \"Machine Learning Engineer\", \"Applications Developer\", \"IT Security Analyst\",\n",
    "            \"Network Engineer\", \"Systems Engineer\", \"Game Developer\", \"IT Business Analyst\", \"Software Architect\", \"Software Test Engineer\", \"UI/UX Developer\", \"Automation Engineer\", \"Performance Analyst\", \"Extended Reality Engineer\"\n",
    "        ],\n",
    "}\n",
    "\n",
    "# Convertir el diccionario a formato JSON\n",
    "json_data = json.dumps(diccionario_datos, indent=4)\n",
    "\n",
    "# Guardar el JSON en un archivo (opcional)\n",
    "with open('datos_scrapers.json', 'w') as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d41eadf-d719-4ab3-b2b5-63d0d64c52d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
