{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20fda4b-208a-42f5-a307-0407b3fe5985",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Paquetes y Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ea6007-a302-430a-a1f5-9bc5491b4b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from funciones_custom import *\n",
    "\n",
    "# Avisos\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2d0ee2-6b8a-4cc9-9670-ad8e850132c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275fd0a-f358-4ee4-b5e9-38a383e64d4a",
   "metadata": {},
   "source": [
    "### API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63564595-23ba-4d13-9ad4-06e5f96b8b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAPS_API_KEY = os.getenv('MAPS_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69da7a5-6605-4563-be20-a8fafd7f7097",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Carga datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c25e6-161b-443c-976f-54db6accb86d",
   "metadata": {},
   "source": [
    "### Rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b78cb1-0155-4813-a26d-86cc4e0950ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ruta_archivos_mapeo = os.getcwd().replace(\"Codigo\\\\Limpieza\", \"Datos\\\\mapping_files\\\\\")\n",
    "ruta_datos_sin_procesar = os.getcwd().replace(\"Codigo\\\\Limpieza\", \"Datos\\\\datos_sin_procesar\\\\\")\n",
    "ruta_datos_procesados = os.getcwd().replace(\"Codigo\\\\Limpieza\", \"Datos\\\\Procesados\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40b5fd-2226-4194-9bc7-7a67a4eb7238",
   "metadata": {},
   "source": [
    "### Datos páginas de empleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "936fc56e-fc83-4760-9b41-b8881cdde7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_linkedin       = pd.read_csv(ruta_datos_sin_procesar + 'datos_linkedin.csv')\n",
    "df_tecnoempleo    = pd.read_csv(ruta_datos_sin_procesar + 'datos_tecnoempleo.csv')\n",
    "df_indeed         = pd.read_csv(ruta_datos_sin_procesar + 'datos_indeed.csv')\n",
    "df_infojobs       = pd.read_csv(ruta_datos_sin_procesar + 'datos_infojobs.csv')\n",
    "df_infoempleo     = pd.read_csv(ruta_datos_sin_procesar + 'datos_infoempleo.csv')\n",
    "df_ticjob         = pd.read_csv(ruta_datos_sin_procesar + 'datos_ticjob.csv')\n",
    "df_talenthacker   = pd.read_csv(ruta_datos_sin_procesar + 'datos_talenthacker.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1eaa2-57ca-480e-a8a5-46eb162115b0",
   "metadata": {},
   "source": [
    "### Archivos de mapeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d132c7b-69af-4b13-80f5-978fb9ce9551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(ruta_archivos_mapeo + \"categorias_empleo.json\", 'r') as archivo_json:\n",
    "    mapping_titulo = json.load(archivo_json)\n",
    "\n",
    "with open(ruta_archivos_mapeo + \"jornadas.json\", 'r') as archivo_json:\n",
    "    mapping_jornada = json.load(archivo_json)\n",
    "\n",
    "with open(ruta_archivos_mapeo + \"contratos.json\", 'r') as archivo_json:\n",
    "    mapping_contrato = json.load(archivo_json)\n",
    "\n",
    "with open(ruta_archivos_mapeo + 'ubicaciones.pickle', 'rb') as archivo:\n",
    "    lista_ubicaciones = pickle.load(archivo)\n",
    "    \n",
    "with open(ruta_archivos_mapeo + 'herramientas.json', 'r') as archivo_json:\n",
    "    dic_herramientas = json.load(archivo_json)\n",
    "    \n",
    "with open(ruta_archivos_mapeo + 'lista_herramientas.pickle', 'rb') as archivo:\n",
    "    lista_herramientas = pickle.load(archivo)\n",
    "\n",
    "with open(ruta_archivos_mapeo + 'empresas.json', 'r') as archivo_json:\n",
    "    dic_empresas = json.load(archivo_json)\n",
    "\n",
    "mapping_presencialidad  = {\n",
    "    r'.*remoto.*'     : 'remoto',\n",
    "    r'.*hibrido.*'    : 'hibrido',\n",
    "    r'.*presencial.*' : 'presencial'}\n",
    "\n",
    "columns = [\"titulo\"        , \"empresa\"      , \"fecha\",\n",
    "           \"herramientas\"  , \"descripcion\"  , \"ubicacion\",\n",
    "           \"presencialidad\", \"funciones\"    , \"jornada\",\n",
    "           \"experiencia\"   , \"tipo_contrato\", \"salario\",\n",
    "           \"beneficios\"    , \"solicitudes\"  , \"fecha_scrapeo\",\n",
    "           \"url\"           , \"portal\"]\n",
    "\n",
    "df_dic = {\"linkedin\"     : df_linkedin,   \"tecnoempleo\": df_tecnoempleo,\n",
    "          \"infojobs\"     : df_infojobs,   \"indeed\"     : df_indeed,\n",
    "          \"infoempleo\"   : df_infoempleo, \"ticjob\"     : df_ticjob,\n",
    "          \"talenthacker\" : df_talenthacker}\n",
    "\n",
    "columnas_numericas = [\"experiencia\", \"solicitudes\"]\n",
    "\n",
    "lista_beneficios = [\"beneficios\", \"pensiones\", \"pension\", \"cheque\", \"transporte\", \"transport\", \"cheques\", \"medical\", \"ticket\", \"tickets\", \"dental\", \"medico\", \"guarderia\", \"childcare\", \"child\", \"vida\", \"health\"]\n",
    "\n",
    "ciudades = ['cordoba', 'cartagena', 'santiago', 'linares', 'ibarra', \"durango\"]\n",
    "\n",
    "pais = ', españa'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46561e5c-1af3-4d6f-a878-cffa72baa069",
   "metadata": {},
   "source": [
    "# 2. Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203504a-060e-4bce-8bda-1f38a86c3535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################################################################################### LIMPIEZA #################################################################################\n",
    "preprocess_dataframes(df_dic, columns, columnas_numericas)\n",
    "\n",
    "##################################################################################### MERGE DATOS ##############################################################################\n",
    "merged_df = pd.concat(df_dic.values())\n",
    "merged_df = merged_df[columns]\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "##################################################################################### LIMPIEZA FECHAS ##########################################################################\n",
    "merged_df['fecha_scrapeo'] = pd.to_datetime(merged_df['fecha_scrapeo'], format='%Y-%m-%d', errors='coerce').dt.date\n",
    "merged_df['fecha'] = merged_df['fecha_scrapeo'] - merged_df['fecha'].apply(date_matching)\n",
    "merged_df['fecha'] = pd.to_datetime(merged_df['fecha'], format='%Y-%m-%d', errors='coerce').dt.date\n",
    "\n",
    "##################################################################################### LIMPIEZA TITULO ##########################################################################\n",
    "merged_df['titulo'], merged_df['categoria_empleo'] = zip(*merged_df['titulo'].apply(lambda x: map_titulo(x, mapping_titulo)))\n",
    "\n",
    "################################################################################# LIMPIEZA PRESENCIALIDAD ######################################################################\n",
    "merged_df['presencialidad'] = merged_df['presencialidad'].apply(lambda x: map_presencialidad(x, mapping_presencialidad))\n",
    "merged_df.loc[merged_df['ubicacion'].isin(['teletrabajo', '100 en remoto']), 'presencialidad'] = 'remoto'\n",
    "merged_df[\"ubicacion\"] = merged_df[\"ubicacion\"].str.replace(r'\\b(para|teletrabajo|100 en remoto)\\b', '', regex=True).replace('', np.nan)\n",
    "\n",
    "##################################################################################### LIMPIEZA JORNADA #########################################################################\n",
    "merged_df['jornada'] = merged_df['jornada'].apply(lambda x: map_jornada(x, mapping_jornada))\n",
    "\n",
    "##################################################################################### LIMPIEZA CONTRATO ########################################################################\n",
    "merged_df['tipo_contrato'] = merged_df['tipo_contrato'].apply(lambda x: map_contrato(x, mapping_contrato))\n",
    "merged_df = merged_df.replace('nan', np.nan)\n",
    "\n",
    "##################################################################################### LIMPIEZA SALARIOS ########################################################################\n",
    "merged_df['salario'] = merged_df['salario'].apply(lambda x: x if re.search(r\"\\d\", str(x)) else np.nan)\n",
    "rellenar_salarios(merged_df)\n",
    "\n",
    "merged_df[\"salario\"] = merged_df[\"salario\"].str.replace(r'\\b(y|a)\\b', '-', regex=True)\n",
    "merged_df[\"salario\"] = merged_df[\"salario\"].str.replace(\"k\", '000')\n",
    "\n",
    "merged_df['salario_min'], merged_df['salario_max'] = zip(*merged_df['salario'].apply(extraer_salarios))\n",
    "\n",
    "##################################################################################### LIMPIEZA BENEFICIOS ######################################################################\n",
    "merged_df[\"beneficios\"] = merged_df.apply(lambda row: buscar_beneficios((str(row['beneficios']) + ', ' + str(row['descripcion'])), lista_beneficios), axis= 1)\n",
    "\n",
    "merged_df = merged_df.map(convert_to_lowercase)\n",
    "\n",
    "##################################################################################### LIMPIEZA HERRAMIENTAS ####################################################################\n",
    "merged_df['herramientas'] = merged_df.apply(lambda row: [herramienta for herramienta in lista_herramientas if buscar_herramienta(str(row['herramientas']) + ', ' + str(row['descripcion']), herramienta)], axis=1)\n",
    "merged_df['herramientas'] = merged_df['herramientas'].apply(lambda herramientas: set(key for herramienta in herramientas for key, values in dic_herramientas.items() if np.isin(herramienta, values, assume_unique = True)))\n",
    "\n",
    "##################################################################################### LIMPIEZA EXPERIENCIA ####################################################################\n",
    "merged_df['experiencia'] = merged_df.apply(extraer_experiencia, axis= 1)\n",
    "merged_df = modificar_experiencia(merged_df)\n",
    "\n",
    "##################################################################################### LIMPIEZA EMPRESAS #######################################################################\n",
    "merged_df['empresa'] = merged_df['empresa'].apply(lambda x: next((key for key, values in dic_empresas.items() if x in values), x))\n",
    "\n",
    "##################################################################################### LIMPIEZA UBICACIONES ####################################################################\n",
    "asignar_ubicaciones(merged_df, lista_ubicaciones)\n",
    "\n",
    "# Limpieza ubicacion con API\n",
    "for ciudad in ciudades:\n",
    "    merged_df.loc[merged_df['ubicacion'].str.contains(ciudad, na=False), 'ubicacion'] += pais\n",
    "\n",
    "# Inicializamos API\n",
    "gmaps = googlemaps.Client(key=MAPS_API_KEY)\n",
    "df_final, ubicaciones_fallidas = obtener_ubicaciones(merged_df, gmaps, MAPS_API_KEY)\n",
    "\n",
    "# Limpieza resultados API\n",
    "df_final['comunidad'], df_final['pais'] = zip(*df_final.apply(agregar_galicia, axis=1))\n",
    "\n",
    "df_final.loc[df_final['provincia'] == 'A Coruña', 'provincia'] = 'La Coruña'\n",
    "df_final.loc[df_final['provincia'] == 'Seville', 'provincia'] = 'Sevilla'\n",
    "df_final.loc[df_final['localidad'] == 'Seville', 'localidad'] = 'Sevilla'\n",
    "\n",
    "df_final.loc[(df_final['latitud'].isnull()) & (df_final['comunidad'] == 'Galicia'), 'latitud'] = 42.755\n",
    "df_final.loc[(df_final['longitud'].isnull()) & (df_final['comunidad'] == 'Galicia'), 'longitud'] = -7.866111\n",
    "\n",
    "##################################################################################### DROPEAR COLUMNAS #########################################################################\n",
    "df_final = df_final.drop([\"funciones\", \"salario\", \"fecha_scrapeo\", \"ubicacion\", \"url\"], axis = 1)\n",
    "\n",
    "filtro_salario = (df_final['salario_min'] > 200_000) | (df_final['salario_max'] > 200_000)\n",
    "df_final.loc[filtro_salario, ['salario_min', 'salario_max']] = np.nan\n",
    "##################################################################################### SAVE #####################################################################################\n",
    "\n",
    "df_final.to_csv(ruta_datos_procesados + 'datos_jobs_finales.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1216093-491a-4f15-8ff3-4fc76a05d58f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
